#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„ Encoder è®­ç»ƒè„šæœ¬

æ”¯æŒå¤šé˜¶æ®µæ¸è¿›å¼è®­ç»ƒï¼š
  - phase1:   å•ä¸€è´å¡å°”æ›²çº¿é‡å»º (åŸºç¡€ç¬”ç”»ç†è§£)
  - phase1.5: è¿ç»­å¤šæ®µè´å¡å°”æ›²çº¿é‡å»º (è¿ç¬”ç†è§£)
  - phase1.6: ç‹¬ç«‹å¤šç¬”ç”»é‡å»º (å¤æ‚å¸ƒå±€ç†è§£)
  - phase1.7: å¤šè·¯å¾„è¿æ¥æ›²çº¿é‡å»º (æ–‡æ¡£çº§ç†è§£)

ä½¿ç”¨æ–¹æ³•:
  python train_encoder.py --phase 1 --from-scratch
  python train_encoder.py --phase 1.5 --resume best_reconstruction.pth
  python train_encoder.py --phase 1.6 --resume best_reconstruction_multi.pth
  python train_encoder.py --phase 1.7 --resume best_reconstruction_independent.pth

ç‰¹æ€§:
  - è‡ªåŠ¨è®¾å¤‡é€‰æ‹©: CUDA > XPU > CPU
  - æ”¯æŒä»é›¶å¼€å§‹æˆ–åŠ è½½æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
  - ä½¿ç”¨ Rust é«˜æ€§èƒ½æ•°æ®ç”Ÿæˆåç«¯
  - TensorBoard æ—¥å¿—è®°å½•
"""

import argparse
import os
import sys
from datetime import datetime
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

import numpy as np
import matplotlib.pyplot as plt

from models import ReconstructionModel
from datasets import InkTraceDataset


# ==================== é…ç½®å®šä¹‰ ====================

PHASE_CONFIGS = {
    "1": {
        "name": "Phase 1: å•æ›²çº¿é‡å»º",
        "mode": "single",
        "checkpoint_name": "best_reconstruction.pth",
        "default_epochs": 50,
        "default_lr": 1e-3,
        "default_batch_size": 32,
        "dataset_params": {},
    },
    "1.5": {
        "name": "Phase 1.5: è¿ç»­å¤šæ®µæ›²çº¿é‡å»º",
        "mode": "continuous",
        "checkpoint_name": "best_reconstruction_multi.pth",
        "default_epochs": 30,
        "default_lr": 5e-4,
        "default_batch_size": 32,
        "dataset_params": {
            "max_segments": 8,
        },
    },
    "1.6": {
        "name": "Phase 1.6: ç‹¬ç«‹å¤šç¬”ç”»é‡å»º",
        "mode": "independent",
        "checkpoint_name": "best_reconstruction_independent.pth",
        "default_epochs": 30,
        "default_lr": 5e-4,
        "default_batch_size": 32,
        "dataset_params": {
            "max_strokes": 8,
        },
    },
    "1.7": {
        "name": "Phase 1.7: å¤šè·¯å¾„è¿æ¥æ›²çº¿é‡å»º",
        "mode": "multi_connected",
        "checkpoint_name": "best_reconstruction_multipath.pth",
        "default_epochs": 30,
        "default_lr": 5e-4,
        "default_batch_size": 32,
        "dataset_params": {
            "max_paths": 4,
            "max_segments": 6,
        },
    },
}


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ç»Ÿä¸€çš„ Encoder è®­ç»ƒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»é›¶å¼€å§‹è®­ç»ƒ Phase 1
  python train_encoder.py --phase 1 --from-scratch

  # ä» Phase 1 æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ Phase 1.5
  python train_encoder.py --phase 1.5 --resume best_reconstruction.pth

  # è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
  python train_encoder.py --phase 1 --epochs 100 --lr 1e-4 --batch-size 64

  # æŒ‡å®šè®¾å¤‡
  python train_encoder.py --phase 1 --device cuda:0
        """,
    )

    # å¿…é¡»å‚æ•°
    parser.add_argument(
        "--phase",
        type=str,
        required=True,
        choices=["1", "1.5", "1.6", "1.7"],
        help="è®­ç»ƒé˜¶æ®µ: 1, 1.5, 1.6, 1.7",
    )

    # é¢„è®¾é…ç½®
    parser.add_argument(
        "--profile",
        type=str,
        choices=["default", "rtx3060", "rtx5090", "tpu"],
        default="default",
        help="ç¡¬ä»¶é…ç½®é¢„è®¾ (default: CPU/è½»é‡çº§, rtx5090: æè‡´æ€§èƒ½)",
    )

    # æ£€æŸ¥ç‚¹å‚æ•° (äº’æ–¥ç»„)
    checkpoint_group = parser.add_mutually_exclusive_group(required=True)
    checkpoint_group.add_argument(
        "--from-scratch",
        action="store_true",
        help="ä»é›¶å¼€å§‹è®­ç»ƒ",
    )
    checkpoint_group.add_argument(
        "--resume",
        type=str,
        metavar="PATH",
        help="ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ",
    )

    # è®­ç»ƒå‚æ•°
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="è®­ç»ƒè½®æ•° (é»˜è®¤æ ¹æ®é˜¶æ®µè‡ªåŠ¨è®¾ç½®)",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=None,
        help="å­¦ä¹ ç‡ (é»˜è®¤æ ¹æ®é˜¶æ®µè‡ªåŠ¨è®¾ç½®)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="æ‰¹æ¬¡å¤§å° (é»˜è®¤æ ¹æ®é˜¶æ®µè‡ªåŠ¨è®¾ç½®)",
    )
    parser.add_argument(
        "--dataset-size",
        type=int,
        default=20000,
        help="æ¯ä¸ª Epoch çš„æ•°æ®é‡ (é»˜è®¤: 20000)",
    )

    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--embed-dim",
        type=int,
        default=128,
        help="Embedding ç»´åº¦ (é»˜è®¤: 128)",
    )
    parser.add_argument(
        "--num-heads",
        type=int,
        default=4,
        help="Transformer æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 4)",
    )
    parser.add_argument(
        "--num-layers",
        type=int,
        default=6,
        help="Transformer å±‚æ•° (é»˜è®¤: 6)",
    )

    # è®¾å¤‡å‚æ•°
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="æŒ‡å®šè®¾å¤‡ (é»˜è®¤: è‡ªåŠ¨é€‰æ‹© cuda > xpu > cpu)",
    )

    # DataLoader å‚æ•°
    parser.add_argument(
        "--num-workers",
        type=int,
        default=4,
        help="DataLoader workers æ•°é‡ (é»˜è®¤: 4)",
    )
    parser.add_argument(
        "--rust-threads",
        type=int,
        default=None,
        help="Rust åç«¯æ¯ä¸ª worker çš„çº¿ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨æ§åˆ¶)",
    )

    # æ—¥å¿—å‚æ•°
    parser.add_argument(
        "--log-dir",
        type=str,
        default="runs",
        help="TensorBoard æ—¥å¿—ç›®å½• (é»˜è®¤: runs)",
    )
    parser.add_argument(
        "--log-interval",
        type=int,
        default=100,
        help="æ—¥å¿—æ‰“å°é—´éš” (batch æ•°, é»˜è®¤: 100)",
    )

    # å…¶ä»–å‚æ•°
    parser.add_argument(
        "--save-interval",
        type=int,
        default=5,
        help="æ£€æŸ¥ç‚¹ä¿å­˜é—´éš” (epoch æ•°, é»˜è®¤: 5)",
    )
    parser.add_argument(
        "--no-visualize",
        action="store_true",
        help="ç¦ç”¨è®­ç»ƒç»“æŸåçš„å¯è§†åŒ–",
    )

    return parser.parse_args()


def get_device(device_arg=None):
    """
    è·å–è®­ç»ƒè®¾å¤‡

    ä¼˜å…ˆçº§: ç”¨æˆ·æŒ‡å®š > CUDA > XPU > CPU
    """
    if device_arg is not None:
        device = torch.device(device_arg)
        print(f"ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")
        return device

    if torch.cuda.is_available():
        device = torch.device("cuda")
        torch.backends.cudnn.benchmark = True
        print(f"ä½¿ç”¨ CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    elif hasattr(torch, "xpu") and torch.xpu.is_available():
        device = torch.device("xpu")
        print(f"ä½¿ç”¨ XPU è®¾å¤‡")
    else:
        device = torch.device("cpu")
        print(f"ä½¿ç”¨ CPU è®¾å¤‡")

    return device


def create_model(embed_dim, num_heads, num_layers, device):
    """åˆ›å»ºæ¨¡å‹"""
    from encoder import StrokeEncoder
    from pixel_decoder import PixelDecoder

    encoder = StrokeEncoder(
        in_channels=1,
        embed_dim=embed_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        dropout=0.1,
    ).to(device)

    pixel_decoder = PixelDecoder(embed_dim=embed_dim).to(device)

    model = ReconstructionModel(encoder, pixel_decoder)
    return model


def load_checkpoint(
    model, checkpoint_path, device, load_optimizer=False, optimizer=None
):
    """
    åŠ è½½æ£€æŸ¥ç‚¹

    Returns:
        start_epoch: èµ·å§‹ epoch
        best_loss: æœ€ä½³æŸå¤±
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")

    print(f"\nåŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # åŠ è½½ encoder æƒé‡
    model.encoder.load_state_dict(checkpoint["encoder_state_dict"])
    print(f"  âœ“ åŠ è½½ Encoder æƒé‡")

    # åŠ è½½ decoder æƒé‡ (å…¼å®¹ä¸åŒçš„ key å)
    if "decoder_state_dict" in checkpoint:
        model.pixel_decoder.load_state_dict(checkpoint["decoder_state_dict"])
        print(f"  âœ“ åŠ è½½ Pixel Decoder æƒé‡")
    elif "pixel_decoder_state_dict" in checkpoint:
        model.pixel_decoder.load_state_dict(checkpoint["pixel_decoder_state_dict"])
        print(f"  âœ“ åŠ è½½ Pixel Decoder æƒé‡")

    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ (å¯é€‰)
    if (
        load_optimizer
        and optimizer is not None
        and "optimizer_state_dict" in checkpoint
    ):
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        print(f"  âœ“ åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€")

    epoch = checkpoint.get("epoch", 0)
    loss = checkpoint.get("loss", float("inf"))
    print(f"  Epoch: {epoch}")
    print(f"  Loss: {loss:.6f}")

    return epoch, loss


def save_checkpoint(model, optimizer, epoch, loss, save_path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        "epoch": epoch,
        "encoder_state_dict": model.encoder.state_dict(),
        "decoder_state_dict": model.pixel_decoder.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "loss": loss,
    }
    torch.save(checkpoint, save_path)


def create_dataset(
    phase_config, dataset_size, batch_size, num_workers, rust_threads=None
):
    """åˆ›å»ºæ•°æ®é›†å’Œ DataLoader"""
    mode = phase_config["mode"]
    dataset_params = phase_config["dataset_params"].copy()

    print(f"\nåˆ›å»ºæ•°æ®é›†...")
    print(f"  æ¨¡å¼: {mode}")
    print(f"  Epoch å¤§å°: {dataset_size}")
    if rust_threads is not None:
        print(f"  Rust Threads: {rust_threads}")
    for k, v in dataset_params.items():
        print(f"  {k}: {v}")

    dataset = InkTraceDataset(
        mode=mode,
        img_size=64,
        batch_size=batch_size,  # Rust å†…éƒ¨ batch
        epoch_length=dataset_size,
        rust_threads=rust_threads,
        **dataset_params,
    )

    # ä½¿ç”¨ IterableDataset æ—¶ï¼Œshuffle åœ¨å†…éƒ¨å¤„ç†
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True,
    )

    return dataset, dataloader


def train_epoch(
    model, dataloader, criterion, optimizer, device, epoch, log_interval, writer=None
):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    epoch_loss = 0.0
    num_batches = 0

    for batch_idx, (imgs, labels) in enumerate(dataloader):
        imgs = imgs.to(device)

        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        reconstructed, embeddings = model(imgs)

        # è®¡ç®—æŸå¤±
        loss = criterion(reconstructed, imgs)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        num_batches += 1

        # æ—¥å¿—
        if (batch_idx + 1) % log_interval == 0:
            print(f"  Batch [{batch_idx + 1}], Loss: {loss.item():.6f}")

        # TensorBoard
        if writer is not None:
            global_step = (epoch - 1) * len(dataloader) + batch_idx
            writer.add_scalar("Loss/batch", loss.item(), global_step)

    avg_loss = epoch_loss / num_batches if num_batches > 0 else float("inf")
    return avg_loss


def visualize_reconstruction(model, dataset, device, save_path, num_samples=6):
    """å¯è§†åŒ–é‡å»ºæ•ˆæœ"""
    model.eval()

    fig, axes = plt.subplots(2, num_samples, figsize=(2 * num_samples, 4))
    fig.suptitle("Reconstruction Results", fontsize=14)

    # ä»æ•°æ®é›†è·å–æ ·æœ¬
    samples = []
    sample_iter = iter(dataset)
    for _ in range(num_samples):
        try:
            img, _ = next(sample_iter)
            samples.append(img)
        except StopIteration:
            break

    if len(samples) == 0:
        print("  âœ— æ— æ³•è·å–æ ·æœ¬è¿›è¡Œå¯è§†åŒ–")
        return

    with torch.no_grad():
        for i, img_tensor in enumerate(samples):
            if i >= num_samples:
                break

            # åŸå›¾
            img = img_tensor.squeeze().numpy() * 255

            # é‡å»º
            img_batch = img_tensor.unsqueeze(0).to(device)
            reconstructed, _ = model(img_batch)
            recon_img = reconstructed.cpu().squeeze().numpy() * 255

            # è®¡ç®—å·®å¼‚
            diff = np.abs(img - recon_img).mean()

            # æ˜¾ç¤ºåŸå›¾
            axes[0, i].imshow(img, cmap="gray", vmin=0, vmax=255)
            axes[0, i].set_title("Original")
            axes[0, i].axis("off")

            # æ˜¾ç¤ºé‡å»ºå›¾
            axes[1, i].imshow(recon_img, cmap="gray", vmin=0, vmax=255)
            axes[1, i].set_title(f"Recon (diff={diff:.1f})")
            axes[1, i].axis("off")

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    print(f"  âœ“ ä¿å­˜å¯è§†åŒ–å›¾åƒ: {save_path}")
    plt.close()


def train(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # 0. åº”ç”¨ Profile é¢„è®¾
    if args.profile == "rtx5090":
        print("\nğŸš€ åº”ç”¨ RTX 5090 æè‡´æ€§èƒ½é…ç½®")
        # æå¤§çš„ Batch Size å’Œ æ•°æ®é‡ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å­˜å’Œç®—åŠ›
        if args.batch_size is None:
            args.batch_size = 1024
        if args.dataset_size == 20000:
            args.dataset_size = 300000  # å¢åŠ åˆ° 30ä¸‡
        # è‡ªåŠ¨å¢åŠ ä¸€ç‚¹ epochs ä»¥ç¡®ä¿åœ¨æµ·é‡æ•°æ®ä¸‹å……åˆ†æ”¶æ•›
        if args.epochs is None:
            # åŸºç¡€ epochs * 1.5
            args.epochs = int(PHASE_CONFIGS[args.phase]["default_epochs"] * 1.5)

    elif args.profile == "rtx3060":
        print("\nğŸš€ åº”ç”¨ RTX 3060 æ€§èƒ½é…ç½®")
        if args.batch_size is None:
            args.batch_size = 256
        if args.dataset_size == 20000:
            args.dataset_size = 100000

    # è·å–é˜¶æ®µé…ç½®
    phase_config = PHASE_CONFIGS[args.phase]
    print("\n" + "=" * 60)
    print(f"  {phase_config['name']}")
    print("=" * 60)

    # è®¾ç½®å‚æ•° (å‘½ä»¤è¡Œ > é»˜è®¤é…ç½®)
    epochs = args.epochs or phase_config["default_epochs"]
    lr = args.lr or phase_config["default_lr"]
    batch_size = args.batch_size or phase_config["default_batch_size"]

    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  Epochs: {epochs}")
    print(f"  Learning Rate: {lr}")
    print(f"  Batch Size: {batch_size}")
    print(f"  Dataset Size: {args.dataset_size}")

    # è·å–è®¾å¤‡
    device = get_device(args.device)

    # åˆ›å»ºæ¨¡å‹
    print(f"\nåˆ›å»ºæ¨¡å‹...")
    model = create_model(args.embed_dim, args.num_heads, args.num_layers, device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # åŠ è½½æ£€æŸ¥ç‚¹ (å¦‚æœæŒ‡å®š)
    start_epoch = 0
    best_loss = float("inf")

    if args.resume:
        load_checkpoint(model, args.resume, device)
        # æ³¨æ„ï¼šç»§ç»­è®­ç»ƒæ—¶ï¼Œä» epoch 0 é‡æ–°å¼€å§‹è®¡æ•°
        # ä½†æƒé‡æ˜¯ä»æ£€æŸ¥ç‚¹åŠ è½½çš„

    # åˆ›å»ºæ•°æ®é›†
    dataset, dataloader = create_dataset(
        phase_config, args.dataset_size, batch_size, args.num_workers, args.rust_threads
    )

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    # TensorBoard
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path(args.log_dir) / f"phase{args.phase}_{timestamp}"
    writer = SummaryWriter(log_dir)
    print(f"\nTensorBoard æ—¥å¿—: {log_dir}")

    # æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„
    checkpoint_path = phase_config["checkpoint_name"]
    checkpoint_dir = Path("checkpoints")
    checkpoint_dir.mkdir(exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    print("\n" + "-" * 60)
    print("å¼€å§‹è®­ç»ƒ...")
    print("-" * 60)

    for epoch in range(1, epochs + 1):
        # è®­ç»ƒä¸€ä¸ª epoch
        avg_loss = train_epoch(
            model,
            dataloader,
            criterion,
            optimizer,
            device,
            epoch,
            args.log_interval,
            writer,
        )

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]["lr"]

        # TensorBoard è®°å½•
        writer.add_scalar("Loss/epoch", avg_loss, epoch)
        writer.add_scalar("LearningRate", current_lr, epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            save_checkpoint(model, optimizer, epoch, best_loss, checkpoint_path)
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ -> {checkpoint_path}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % args.save_interval == 0:
            periodic_path = checkpoint_dir / f"phase{args.phase}_epoch{epoch}.pth"
            save_checkpoint(model, optimizer, epoch, avg_loss, periodic_path)

        # æ‰“å° epoch ç»Ÿè®¡
        print(f"\nEpoch {epoch}/{epochs}")
        print(f"  å¹³å‡æŸå¤±: {avg_loss:.6f}")
        print(f"  æœ€ä½³æŸå¤±: {best_loss:.6f}")
        print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
        print("-" * 60)

    writer.close()

    # è®­ç»ƒå®Œæˆ
    print("\n" + "=" * 60)
    print(f"{phase_config['name']} è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³æŸå¤±: {best_loss:.6f}")
    print(f"æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}")
    print("=" * 60)

    # å¯è§†åŒ–
    if not args.no_visualize:
        print("\nç”Ÿæˆå¯è§†åŒ–...")
        vis_path = f"reconstruction_phase{args.phase}.png"
        # åˆ›å»ºæ–°çš„æ•°æ®é›†ç”¨äºå¯è§†åŒ– (é¿å…è¿­ä»£å™¨è€—å°½é—®é¢˜)
        vis_dataset = InkTraceDataset(
            mode=phase_config["mode"],
            img_size=64,
            batch_size=16,
            epoch_length=100,
            **phase_config["dataset_params"],
        )
        visualize_reconstruction(model, vis_dataset, device, vis_path)

    return model


def main():
    """å…¥å£å‡½æ•°"""
    args = parse_args()

    print("\n" + "=" * 60)
    print("  InkTrace Encoder Training")
    print("  æ‰‹å†™æ–‡å­—é«˜ä¿çœŸçŸ¢é‡åŒ– - Encoder è®­ç»ƒ")
    print("=" * 60)

    try:
        train(args)
        print("\nè®­ç»ƒæˆåŠŸå®Œæˆ!")
    except KeyboardInterrupt:
        print("\n\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nè®­ç»ƒå‡ºé”™: {e}")
        raise


if __name__ == "__main__":
    main()
