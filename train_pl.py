#!/usr/bin/env python3
"""
PyTorch Lightning è®­ç»ƒå…¥å£è„šæœ¬

ç‰¹æ€§ï¼š
- æ·±åº¦é…ç½®åˆå¹¶ï¼šé»˜è®¤é…ç½® + é˜¶æ®µè¦†ç›–é…ç½®
- è‡ªåŠ¨å¤„ç†æ— é™æ•°æ®é›†ä¸ OneCycleLR çš„å…¼å®¹é—®é¢˜
- è‡ªåŠ¨ Checkpoint ç®¡ç† (Top-K & Last)
- æ”¯æŒå¤šé˜¶æ®µè®­ç»ƒæµæ°´çº¿ (structural -> dense -> finetune)
- Curriculum Learning æ”¯æŒ
- æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
- TensorBoard æ—¥å¿—

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•é˜¶æ®µè®­ç»ƒ (structural pretrain)
    python train_pl.py --config configs/default.yaml --stage structural

    # å•é˜¶æ®µè®­ç»ƒ (dense)
    python train_pl.py --config configs/default.yaml --stage dense

    # Dense è®­ç»ƒå¹¶ä» structural checkpoint åˆå§‹åŒ–
    python train_pl.py --config configs/default.yaml --stage dense \\
        --init_from checkpoints/structural/last.ckpt

    # å¤šé˜¶æ®µè‡ªåŠ¨è®­ç»ƒ
    python train_pl.py --config configs/default.yaml --run-all-stages

    # æ–­ç‚¹ç»­è®­
    python train_pl.py --config configs/default.yaml --stage dense \\
        --resume checkpoints/dense/last.ckpt
"""

import argparse
import copy
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

import yaml
import pytorch_lightning as pl
from pytorch_lightning.callbacks import (
    ModelCheckpoint,
    LearningRateMonitor,
    RichProgressBar,
)
from pytorch_lightning.loggers import TensorBoardLogger

from lightning_model import UnifiedTask, CurriculumCallback
from lightning_data import InkTraceDataModule
from lightning_vis import VisualizationCallback


# =============================================================================
# é…ç½®å·¥å…·å‡½æ•°
# =============================================================================


def deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ·±åº¦åˆå¹¶ä¸¤ä¸ªå­—å…¸ï¼Œoverride ä¸­çš„å€¼ä¼šè¦†ç›– base ä¸­çš„å€¼

    Args:
        base: åŸºç¡€å­—å…¸
        override: è¦†ç›–å­—å…¸

    Returns:
        åˆå¹¶åçš„æ–°å­—å…¸
    """
    result = copy.deepcopy(base)

    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            # é€’å½’åˆå¹¶åµŒå¥—å­—å…¸
            result[key] = deep_merge(result[key], value)
        else:
            # ç›´æ¥è¦†ç›–
            result[key] = copy.deepcopy(value)

    return result


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_stage_config(config: Dict[str, Any], stage_name: str) -> Dict[str, Any]:
    """
    è·å–ç‰¹å®šé˜¶æ®µçš„å®Œæ•´é…ç½®

    åŸç†ï¼š
    1. ä»å…¨å±€é…ç½®ä¸­æå–é»˜è®¤å€¼
    2. ä» stages[stage_name] ä¸­æå–é˜¶æ®µè¦†ç›–é…ç½®
    3. æ·±åº¦åˆå¹¶ä¸¤è€…

    Args:
        config: å…¨å±€é…ç½®
        stage_name: é˜¶æ®µåç§° ("structural", "dense", "finetune", "debug")

    Returns:
        åˆå¹¶åçš„é˜¶æ®µé…ç½®
    """
    # 1. æå–å…¨å±€é»˜è®¤é…ç½®
    defaults = {
        "model": config.get("model", {}),
        "training": config.get("training", {}),
        "data": config.get("data", {}),
        "logging": config.get("logging", {}),
        "device": config.get("device", {}),
        "curriculum": config.get("curriculum", {}),
    }

    # 2. è·å–é˜¶æ®µç‰¹å®šé…ç½®
    stages = config.get("stages", {})
    if stage_name not in stages:
        print(f"âš ï¸  Stage '{stage_name}' not found in config, using defaults")
        return defaults

    stage_override = stages[stage_name]

    # 3. æ·±åº¦åˆå¹¶
    merged = deep_merge(defaults, stage_override)

    # 4. å¤„ç†ç‰¹æ®Šå­—æ®µ
    # - training.curriculum è¦†ç›–å…¨å±€ curriculum
    if "curriculum" in stage_override.get("training", {}):
        merged["curriculum"] = deep_merge(
            merged.get("curriculum", {}), stage_override["training"]["curriculum"]
        )

    # - é˜¶æ®µçº§åˆ«çš„ init_from å’Œ freeze_encoder
    merged["init_from"] = stage_override.get("init_from")
    merged["freeze_encoder"] = stage_override.get("freeze_encoder", False)
    merged["description"] = stage_override.get("description", "")

    return merged


def validate_config(config: Dict[str, Any], stage_name: str) -> None:
    """éªŒè¯é…ç½®æœ‰æ•ˆæ€§å¹¶ç¡®ä¿ç±»å‹æ­£ç¡®"""
    training = config.get("training", {})
    model = config.get("model", {})
    data = config.get("data", {})

    # =========================================================================
    # éªŒè¯ training é…ç½®
    # =========================================================================
    required_training = ["lr", "epochs", "batch_size", "epoch_length"]
    for field in required_training:
        if field not in training:
            raise ValueError(f"Missing required training config: {field}")

    # æ•°å€¼èŒƒå›´æ£€æŸ¥ï¼ˆç¡®ä¿ç±»å‹è½¬æ¢ï¼‰
    lr = float(training["lr"])
    batch_size = int(training["batch_size"])
    epochs = int(training["epochs"])
    epoch_length = int(training["epoch_length"])
    grad_clip = float(training.get("grad_clip", 1.0))
    weight_decay = float(training.get("weight_decay", 1e-4))
    mask_ratio = float(training.get("mask_ratio", 0.6))

    if lr <= 0:
        raise ValueError(f"Learning rate must be positive, got {lr}")
    if batch_size <= 0:
        raise ValueError(f"Batch size must be positive, got {batch_size}")
    if epochs <= 0:
        raise ValueError(f"Epochs must be positive, got {epochs}")
    if epoch_length <= 0:
        raise ValueError(f"Epoch length must be positive, got {epoch_length}")
    if grad_clip < 0:
        raise ValueError(f"Grad clip must be non-negative, got {grad_clip}")
    if weight_decay < 0:
        raise ValueError(f"Weight decay must be non-negative, got {weight_decay}")
    if not 0 <= mask_ratio <= 1:
        raise ValueError(f"Mask ratio must be in [0, 1], got {mask_ratio}")

    # æ›´æ–° training é…ç½®ä¸ºæ­£ç¡®çš„ç±»å‹
    training["lr"] = lr
    training["batch_size"] = batch_size
    training["epochs"] = epochs
    training["epoch_length"] = epoch_length
    training["grad_clip"] = grad_clip
    training["weight_decay"] = weight_decay
    training["mask_ratio"] = mask_ratio

    # éªŒè¯ scheduler é…ç½®
    scheduler = training.get("scheduler", {})
    if "warmup_epochs" in scheduler:
        training["scheduler"]["warmup_epochs"] = int(scheduler["warmup_epochs"])
    if "pct_start" in scheduler:
        training["scheduler"]["pct_start"] = float(scheduler["pct_start"])

    # =========================================================================
    # éªŒè¯ model é…ç½®
    # =========================================================================
    if "embed_dim" in model:
        model["embed_dim"] = int(model["embed_dim"])
        if model["embed_dim"] <= 0:
            raise ValueError(f"embed_dim must be positive, got {model['embed_dim']}")
    if "num_layers" in model:
        model["num_layers"] = int(model["num_layers"])
        if model["num_layers"] <= 0:
            raise ValueError(f"num_layers must be positive, got {model['num_layers']}")

    # =========================================================================
    # éªŒè¯ data é…ç½®
    # =========================================================================
    if "img_size" in data:
        data["img_size"] = int(data["img_size"])
        if data["img_size"] <= 0:
            raise ValueError(f"img_size must be positive, got {data['img_size']}")
    if "curriculum_stage" in data:
        data["curriculum_stage"] = int(data["curriculum_stage"])
    if "num_workers" in data:
        data["num_workers"] = int(data["num_workers"])
        if data["num_workers"] < 0:
            raise ValueError(f"num_workers must be non-negative, got {data['num_workers']}")
    if "keypoint_sigma" in data:
        data["keypoint_sigma"] = float(data["keypoint_sigma"])
        if data["keypoint_sigma"] <= 0:
            raise ValueError(f"keypoint_sigma must be positive, got {data['keypoint_sigma']}")

    print(f"âœ… Config validation passed for stage: {stage_name}")


def print_stage_config(config: Dict[str, Any], stage_name: str) -> None:
    """æ‰“å°é˜¶æ®µé…ç½®æ‘˜è¦"""
    training = config.get("training", {})
    model = config.get("model", {})
    data = config.get("data", {})
    curriculum = config.get("curriculum", {})

    print(f"\n{'â”€' * 60}")
    print(f"ğŸ“‹ Stage: {stage_name}")
    if config.get("description"):
        print(f"   {config['description']}")
    print(f"{'â”€' * 60}")

    print("  Model:")
    print(f"    embed_dim: {model.get('embed_dim', 128)}")
    print(f"    num_layers: {model.get('num_layers', 4)}")
    print(f"    full_heads: {model.get('full_heads', True)}")

    print("  Training:")
    print(f"    lr: {training.get('lr')}")
    print(f"    epochs: {training.get('epochs')}")
    print(f"    batch_size: {training.get('batch_size')}")
    print(f"    epoch_length: {training.get('epoch_length')}")

    print("  Data:")
    print(f"    curriculum_stage: {data.get('curriculum_stage', 0)}")
    print(f"    num_workers: {data.get('num_workers', 8)}")

    if curriculum.get("enabled"):
        print("  Curriculum Learning:")
        print(
            f"    stages: {curriculum.get('start_stage', 0)} -> {curriculum.get('end_stage', 6)}"
        )
        print(f"    epochs_per_stage: {curriculum.get('epochs_per_stage', 10)}")

    if config.get("init_from"):
        print(f"  Init from: {config['init_from']}")
    if config.get("freeze_encoder"):
        print("  Freeze encoder: True")

    print(f"{'â”€' * 60}\n")


# =============================================================================
# Trainer å·¥å‚
# =============================================================================


def create_trainer(
    config: Dict[str, Any],
    stage_name: str,
    resume_from: Optional[str] = None,
) -> pl.Trainer:
    """
    åˆ›å»ºé…ç½®å¥½çš„ Trainer

    Args:
        config: é˜¶æ®µé…ç½®ï¼ˆå·²åˆå¹¶çš„å®Œæ•´é…ç½®ï¼‰
        stage_name: é˜¶æ®µåç§°
        resume_from: æ–­ç‚¹ç»­è®­ checkpoint è·¯å¾„

    Returns:
        é…ç½®å¥½çš„ pl.Trainer
    """
    training_config = config.get("training", {})
    logging_config = config.get("logging", {})
    device_config = config.get("device", {})
    curriculum_config = config.get("curriculum", {})

    # =========================================================================
    # æ ¸å¿ƒä¿®å¤: limit_train_batches
    # å¯¹äºæ— é™æ•°æ®é›†ï¼Œå¿…é¡»è®¾ç½®æ­¤å‚æ•°æ¥å®šä¹‰æ¯ä¸ª epoch çš„ batch æ•°é‡
    # è¿™æ · OneCycleLR æ‰èƒ½æ­£ç¡®è®¡ç®— total_steps
    # =========================================================================
    epoch_length = int(training_config.get("epoch_length", 10000))
    batch_size = int(training_config.get("batch_size", 128))
    limit_train_batches = epoch_length // batch_size

    # TensorBoard Logger
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logging_config.get("tensorboard_dir", "runs")
    logger = TensorBoardLogger(
        save_dir=log_dir,
        name=stage_name,
        version=timestamp,
    )

    # =========================================================================
    # Callbacks
    # =========================================================================
    callbacks = []

    # 1. ModelCheckpoint - ä¿å­˜ Top-K å’Œ Last
    checkpoint_config = training_config.get("checkpoint", {})
    save_dir = Path(checkpoint_config.get("save_dir", f"checkpoints/{stage_name}"))
    save_dir.mkdir(parents=True, exist_ok=True)

    monitor_metric = checkpoint_config.get("monitor", "val/loss")
    monitor_mode = checkpoint_config.get("mode", "min")

    # å¯¹äº structural é˜¶æ®µï¼Œç›‘æ§ train/lossï¼ˆæ— éªŒè¯é›†ï¼‰
    if stage_name == "structural":
        monitor_metric = checkpoint_config.get("monitor", "train/loss")

    checkpoint_callback = ModelCheckpoint(
        dirpath=str(save_dir),
        filename="epoch{epoch:02d}-{" + monitor_metric.replace("/", "_") + ":.4f}",
        save_top_k=checkpoint_config.get("keep_top_k", 3),
        monitor=monitor_metric,
        mode=monitor_mode,
        save_last=checkpoint_config.get("save_last", True),
        auto_insert_metric_name=False,
    )
    callbacks.append(checkpoint_callback)

    # 2. LearningRateMonitor
    lr_monitor = LearningRateMonitor(logging_interval="step")
    callbacks.append(lr_monitor)

    # 3. RichProgressBar (å¯é€‰)
    try:
        callbacks.append(RichProgressBar())
    except Exception:
        pass

    # 4. Curriculum Learning Callback
    if curriculum_config.get("enabled", False):
        curriculum_callback = CurriculumCallback(
            start_stage=int(curriculum_config.get("start_stage", 0)),
            end_stage=int(curriculum_config.get("end_stage", 6)),
            epochs_per_stage=int(curriculum_config.get("epochs_per_stage", 10)),
        )
        callbacks.append(curriculum_callback)
        print(
            f"ğŸ“ˆ Curriculum Learning: "
            f"stage {curriculum_config.get('start_stage', 0)} -> "
            f"{curriculum_config.get('end_stage', 6)}, "
            f"{curriculum_config.get('epochs_per_stage', 10)} epochs/stage"
        )

    # 5. Visualization Callback
    vis_config = training_config.get("visualization", {})
    if vis_config.get("enabled", True):
        vis_callback = VisualizationCallback(
            num_samples=int(vis_config.get("num_samples", 4)),
            log_metrics=vis_config.get("log_metrics", True),
            log_interval=int(vis_config.get("log_interval", 1)),
            prefix="Validation" if stage_name != "structural" else "Train",
        )
        callbacks.append(vis_callback)
        print(f"ğŸ¨ Visualization: {vis_config.get('num_samples', 4)} samples")

    # =========================================================================
    # Trainer é…ç½®
    # =========================================================================

    # è®¾å¤‡é…ç½®
    accelerator = device_config.get("accelerator", "auto")
    precision = device_config.get("precision", "16-mixed")

    # å…¼å®¹æ—§é…ç½®æ ¼å¼
    if "type" in device_config:
        device_type = device_config["type"]
        if device_type == "cuda":
            accelerator = "gpu"
        elif device_type == "cpu":
            accelerator = "cpu"
            precision = "32"
        elif device_type == "xpu":
            accelerator = "xpu"

    # CPU æ¨¡å¼ä¸‹ä½¿ç”¨ 32 ç²¾åº¦
    if accelerator == "cpu":
        precision = "32"

    trainer = pl.Trainer(
        # åŸºç¡€é…ç½®
        max_epochs=int(training_config.get("epochs", 50)),
        accelerator=accelerator,
        devices="auto",
        precision=precision,
        # æ ¸å¿ƒ: é™åˆ¶æ¯ä¸ª epoch çš„ batch æ•°é‡
        limit_train_batches=limit_train_batches,
        limit_val_batches=max(1, limit_train_batches // 10),
        # æ¢¯åº¦è£å‰ª
        gradient_clip_val=float(training_config.get("grad_clip", 1.0)),
        # Callbacks & Logger
        callbacks=callbacks,
        logger=logger,
        # æ—¥å¿—é¢‘ç‡
        log_every_n_steps=int(logging_config.get("log_interval", 10)),
        # éªŒè¯é¢‘ç‡
        check_val_every_n_epoch=1,
        # å…³é—­ sanity check
        num_sanity_val_steps=0,
        # æ€§èƒ½ä¼˜åŒ–
        enable_model_summary=True,
        enable_progress_bar=True,
    )

    return trainer


# =============================================================================
# è®­ç»ƒå‡½æ•°
# =============================================================================


def run_stage(
    config: Dict[str, Any],
    stage_name: str,
    init_from: Optional[str] = None,
    resume_from: Optional[str] = None,
) -> str:
    """
    è¿è¡Œå•ä¸ªè®­ç»ƒé˜¶æ®µ

    Args:
        config: å…¨å±€é…ç½®
        stage_name: é˜¶æ®µåç§° ("structural", "dense", "finetune", "debug")
        init_from: åˆå§‹åŒ–æƒé‡è·¯å¾„ (ç”¨äºè¿ç§»å­¦ä¹ )
        resume_from: æ–­ç‚¹ç»­è®­ checkpoint è·¯å¾„

    Returns:
        æœ€ä½³ checkpoint è·¯å¾„
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸš€ Starting stage: {stage_name}")
    print(f"{'=' * 60}")

    # è·å–é˜¶æ®µé…ç½®
    stage_config = get_stage_config(config, stage_name)

    # éªŒè¯å’Œæ‰“å°é…ç½®
    validate_config(stage_config, stage_name)
    print_stage_config(stage_config, stage_name)

    training_config = stage_config.get("training", {})
    model_config = stage_config.get("model", {})
    data_config = stage_config.get("data", {})

    # å¤„ç† init_from
    # ä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > é˜¶æ®µé…ç½® > None
    effective_init_from = init_from
    if effective_init_from is None:
        effective_init_from = stage_config.get("init_from")

    # å¤„ç† "auto" ç‰¹æ®Šå€¼ï¼ˆç”± run_all_stages å¡«å……ï¼‰
    if effective_init_from == "auto":
        effective_init_from = None  # åç»­ç”± pipeline å¤„ç†

    # =========================================================================
    # åˆ›å»º DataModule
    # =========================================================================
    datamodule = InkTraceDataModule(
        img_size=int(data_config.get("img_size", 64)),
        batch_size=int(training_config.get("batch_size", 128)),
        epoch_length=int(training_config.get("epoch_length", 10000)),
        curriculum_stage=int(data_config.get("curriculum_stage", 0)),
        num_workers=int(data_config.get("num_workers", 8)),
        rust_threads=data_config.get("rust_threads"),
        pin_memory=data_config.get("pin_memory", True),
        persistent_workers=data_config.get("persistent_workers", True),
        keypoint_sigma=float(data_config.get("keypoint_sigma", 1.5)),
    )

    # =========================================================================
    # åˆ›å»ºæ¨¡å‹
    # =========================================================================
    scheduler_config = training_config.get("scheduler", {})

    model = UnifiedTask(
        stage=stage_name if stage_name != "debug" else "dense",
        embed_dim=int(model_config.get("embed_dim", 192)),
        num_layers=int(model_config.get("num_layers", 4)),
        lr=float(training_config.get("lr", 1e-3)),
        weight_decay=float(training_config.get("weight_decay", 1e-4)),
        loss_weights=training_config.get("loss_weights"),
        mask_ratio=float(model_config.get("mask_ratio", 0.6)),
        mask_strategy=model_config.get("mask_strategy", "block"),
        grad_clip=float(training_config.get("grad_clip", 1.0)),
        scheduler_type=scheduler_config.get("type", "onecycle"),
        warmup_epochs=int(scheduler_config.get("warmup_epochs", 2)),
        pct_start=float(scheduler_config.get("pct_start", 0.1)),
    )

    # ä» checkpoint åˆå§‹åŒ–æƒé‡ (è¿ç§»å­¦ä¹ )
    if effective_init_from and not resume_from:
        freeze_encoder = stage_config.get("freeze_encoder", False)
        model.load_pretrained_weights(
            effective_init_from, strict=False, freeze_encoder=freeze_encoder
        )

    # =========================================================================
    # åˆ›å»º Trainer
    # =========================================================================
    trainer = create_trainer(stage_config, stage_name, resume_from)

    # =========================================================================
    # å¼€å§‹è®­ç»ƒ
    # =========================================================================
    trainer.fit(
        model,
        datamodule=datamodule,
        ckpt_path=resume_from,
    )

    # è¿”å›æœ€ä½³ checkpoint è·¯å¾„
    best_ckpt = trainer.checkpoint_callback.best_model_path
    print(f"\nâœ… Stage {stage_name} completed!")
    print(f"   Best checkpoint: {best_ckpt}")

    return best_ckpt


def run_all_stages(config: Dict[str, Any], start_stage: Optional[str] = None) -> str:
    """
    è¿è¡Œå®Œæ•´çš„å¤šé˜¶æ®µè®­ç»ƒæµæ°´çº¿

    æŒ‰ pipeline.order ä¸­å®šä¹‰çš„é¡ºåºæ‰§è¡Œå„é˜¶æ®µ
    è‡ªåŠ¨åœ¨é˜¶æ®µä¹‹é—´ä¼ é€’æƒé‡

    Args:
        config: å…¨å±€é…ç½®
        start_stage: ä»å“ªä¸ªé˜¶æ®µå¼€å§‹ (ç”¨äºæ¢å¤è®­ç»ƒ)

    Returns:
        æœ€ç»ˆæœ€ä¼˜ checkpoint è·¯å¾„
    """
    pipeline_config = config.get("pipeline", {})
    stage_order = pipeline_config.get("order", ["structural", "dense"])
    auto_transfer = pipeline_config.get("auto_transfer", True)

    print(f"\n{'#' * 60}")
    print("ğŸ¯ Multi-Stage Training Pipeline")
    print(f"   Stages: {' -> '.join(stage_order)}")
    print(f"   Auto transfer: {auto_transfer}")
    print(f"{'#' * 60}")

    # ç¡®å®šèµ·å§‹ç‚¹
    start_idx = 0
    if start_stage:
        try:
            start_idx = stage_order.index(start_stage)
            print(f"ğŸ“ Starting from stage: {start_stage}")
        except ValueError:
            print(f"âš ï¸  Stage '{start_stage}' not in pipeline, starting from beginning")

    last_ckpt = None

    for idx, stage_name in enumerate(stage_order):
        if idx < start_idx:
            continue

        # ç¡®å®š init_from
        init_from = None
        if auto_transfer and last_ckpt:
            init_from = last_ckpt
            print(f"\nğŸ”— Transferring weights from: {last_ckpt}")

        # æ£€æŸ¥é˜¶æ®µé…ç½®ä¸­çš„ init_from
        stage_config = get_stage_config(config, stage_name)
        stage_init = stage_config.get("init_from")
        if stage_init and stage_init != "auto":
            init_from = stage_init

        # è¿è¡Œé˜¶æ®µ
        best_ckpt = run_stage(config, stage_name, init_from=init_from)
        last_ckpt = best_ckpt

    print(f"\n{'#' * 60}")
    print("ğŸ‰ All stages completed!")
    print(f"   Final checkpoint: {last_ckpt}")
    print(f"{'#' * 60}\n")

    return last_ckpt


# =============================================================================
# CLI
# =============================================================================


def parse_args():
    parser = argparse.ArgumentParser(
        description="InkTrace PyTorch Lightning Training",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single stage training
  python train_pl.py --config configs/default.yaml --stage structural
  python train_pl.py --config configs/default.yaml --stage dense

  # Multi-stage pipeline
  python train_pl.py --config configs/default.yaml --run-all-stages

  # Resume training
  python train_pl.py --config configs/default.yaml --stage dense --resume checkpoints/dense/last.ckpt

  # Transfer learning
  python train_pl.py --config configs/default.yaml --stage dense --init_from checkpoints/structural/best.ckpt

  # Quick debug
  python train_pl.py --config configs/default.yaml --stage debug
        """,
    )

    # é…ç½®
    parser.add_argument(
        "--config",
        type=str,
        default="configs/default.yaml",
        help="YAML é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--stage",
        type=str,
        help="è®­ç»ƒé˜¶æ®µ (structural, dense, finetune, debug æˆ–å…¶ä»–è‡ªå®šä¹‰é˜¶æ®µ)",
    )
    parser.add_argument(
        "--run-all-stages",
        action="store_true",
        help="è¿è¡Œ pipeline ä¸­å®šä¹‰çš„æ‰€æœ‰é˜¶æ®µ",
    )
    parser.add_argument(
        "--start-from",
        type=str,
        help="å¤šé˜¶æ®µè®­ç»ƒæ—¶ä»å“ªä¸ªé˜¶æ®µå¼€å§‹ (ç”¨äºæ¢å¤)",
    )

    # Checkpoint
    parser.add_argument(
        "--init_from",
        type=str,
        help="ä» checkpoint åˆå§‹åŒ–æ¨¡å‹ (è¿ç§»å­¦ä¹ )",
    )
    parser.add_argument(
        "--resume",
        type=str,
        help="ä» checkpoint æ–­ç‚¹ç»­è®­",
    )

    # è¦†ç›–é…ç½® (å¯é€‰)
    parser.add_argument("--lr", type=float, help="è¦†ç›–å­¦ä¹ ç‡")
    parser.add_argument("--epochs", type=int, help="è¦†ç›–è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, help="è¦†ç›–æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--seed", type=int, default=114514, help="éšæœºç§å­")

    return parser.parse_args()


def main():
    args = parse_args()

    # åŠ è½½é…ç½®
    config = load_config(args.config)

    # CLI å‚æ•°è¦†ç›–é…ç½®
    if args.lr:
        config["training"]["lr"] = args.lr
    if args.epochs:
        config["training"]["epochs"] = args.epochs
    if args.batch_size:
        config["training"]["batch_size"] = args.batch_size

    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed, workers=True)

    # è¿è¡Œè®­ç»ƒ
    if args.run_all_stages:
        run_all_stages(config, start_stage=args.start_from)
    elif args.stage:
        run_stage(
            config,
            args.stage,
            init_from=args.init_from,
            resume_from=args.resume,
        )
    else:
        # é»˜è®¤åˆ—å‡ºå¯ç”¨é˜¶æ®µ
        stages = config.get("stages", {})
        print("\nå¯ç”¨çš„è®­ç»ƒé˜¶æ®µ:")
        for name, stage_config in stages.items():
            desc = stage_config.get("description", "")
            print(f"  - {name}: {desc}")
        print("\nä½¿ç”¨ --stage <name> æŒ‡å®šé˜¶æ®µï¼Œæˆ– --run-all-stages è¿è¡Œå®Œæ•´æµæ°´çº¿\n")


if __name__ == "__main__":
    main()
