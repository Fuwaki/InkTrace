#!/usr/bin/env python3
"""
PyTorch Lightning è®­ç»ƒå…¥å£è„šæœ¬

ç‰¹æ€§ï¼š
- è‡ªåŠ¨å¤„ç†æ— é™æ•°æ®é›†ä¸ OneCycleLR çš„å…¼å®¹é—®é¢˜
- è‡ªåŠ¨ Checkpoint ç®¡ç† (Top-K & Last)
- æ”¯æŒå¤šé˜¶æ®µè®­ç»ƒæµæ°´çº¿ (structural -> dense)
- Curriculum Learning æ”¯æŒ
- æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
- TensorBoard æ—¥å¿—

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•é˜¶æ®µè®­ç»ƒ (structural pretrain)
    python train_pl.py --config configs/default.yaml --stage structural

    # å•é˜¶æ®µè®­ç»ƒ (dense)
    python train_pl.py --config configs/default.yaml --stage dense

    # Dense è®­ç»ƒå¹¶ä» structural checkpoint åˆå§‹åŒ–
    python train_pl.py --config configs/default.yaml --stage dense \\
        --init_from checkpoints/structural/last.ckpt

    # å¤šé˜¶æ®µè‡ªåŠ¨è®­ç»ƒ
    python train_pl.py --config configs/default.yaml --run-all-stages

    # æ–­ç‚¹ç»­è®­
    python train_pl.py --config configs/default.yaml --stage dense \\
        --resume checkpoints/dense/last.ckpt
"""

import argparse
from datetime import datetime
from pathlib import Path
from typing import Optional

import yaml
import pytorch_lightning as pl
from pytorch_lightning.callbacks import (
    ModelCheckpoint,
    LearningRateMonitor,
    RichProgressBar,
)
from pytorch_lightning.loggers import TensorBoardLogger

from lightning_model import UnifiedTask, CurriculumCallback
from lightning_data import InkTraceDataModule
from lightning_vis import VisualizationCallback


# =============================================================================
# é…ç½®åŠ è½½
# =============================================================================


def load_config(config_path: str) -> dict:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def get_stage_config(config: dict, stage_name: str) -> dict:
    """
    è·å–ç‰¹å®šé˜¶æ®µçš„é…ç½®ï¼Œåˆå¹¶å…¨å±€é…ç½®

    Args:
        config: å…¨å±€é…ç½®
        stage_name: é˜¶æ®µåç§° ("structural" æˆ– "dense")

    Returns:
        åˆå¹¶åçš„é˜¶æ®µé…ç½®
    """
    # å¤åˆ¶åŸºç¡€é…ç½®
    stage_config = {
        "model": dict(config.get("model", {})),
        "training": dict(config.get("training", {})),
        "data": dict(config.get("data", {})),
        "logging": dict(config.get("logging", {})),
        "device": dict(config.get("device", {})),
    }

    # æŸ¥æ‰¾å¹¶åˆå¹¶é˜¶æ®µç‰¹å®šé…ç½®
    stages = config.get("stages", [])
    for stage in stages:
        if stage.get("name") == stage_name:
            # åˆå¹¶æ¨¡å‹é…ç½®
            if "model" in stage:
                stage_config["model"].update(stage["model"])
            # åˆå¹¶è®­ç»ƒé…ç½®
            if "training" in stage:
                stage_config["training"].update(stage["training"])
            # åˆå¹¶æ•°æ®é…ç½®
            if "data" in stage:
                stage_config["data"].update(stage["data"])
            # é˜¶æ®µç‰¹å®šçš„ epochs
            if "epochs" in stage:
                stage_config["training"]["epochs"] = stage["epochs"]
            # é˜¶æ®µç‰¹å®šçš„ init_from
            if "init_from" in stage:
                stage_config["init_from"] = stage["init_from"]
            break

    return stage_config


# =============================================================================
# Trainer å·¥å‚
# =============================================================================


def create_trainer(
    config: dict,
    stage_name: str,
    resume_from: Optional[str] = None,
) -> pl.Trainer:
    """
    åˆ›å»ºé…ç½®å¥½çš„ Trainer

    Args:
        config: é˜¶æ®µé…ç½®
        stage_name: é˜¶æ®µåç§°
        resume_from: æ–­ç‚¹ç»­è®­ checkpoint è·¯å¾„

    Returns:
        é…ç½®å¥½çš„ pl.Trainer
    """
    training_config = config.get("training", {})
    logging_config = config.get("logging", {})
    device_config = config.get("device", {})

    # =========================================================================
    # æ ¸å¿ƒä¿®å¤: limit_train_batches
    # å¯¹äºæ— é™æ•°æ®é›†ï¼Œå¿…é¡»è®¾ç½®æ­¤å‚æ•°æ¥å®šä¹‰æ¯ä¸ª epoch çš„ batch æ•°é‡
    # è¿™æ · OneCycleLR æ‰èƒ½æ­£ç¡®è®¡ç®— total_steps
    # =========================================================================
    epoch_length = training_config.get("epoch_length", 10000)
    batch_size = training_config.get("batch_size", 128)
    limit_train_batches = epoch_length // batch_size

    # TensorBoard Logger
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logging_config.get("tensorboard_dir", "runs")
    logger = TensorBoardLogger(
        save_dir=log_dir,
        name=stage_name,
        version=timestamp,
    )

    # =========================================================================
    # Callbacks
    # =========================================================================
    callbacks = []

    # 1. ModelCheckpoint - ä¿å­˜ Top-K å’Œ Last
    save_dir = Path(training_config.get("save_dir", "checkpoints")) / stage_name
    save_dir.mkdir(parents=True, exist_ok=True)

    checkpoint_callback = ModelCheckpoint(
        dirpath=str(save_dir),
        filename="epoch{epoch:02d}-loss{train/loss:.4f}",
        save_top_k=training_config.get("keep_last_n", 3),
        monitor="train/loss",
        mode="min",
        save_last=True,  # å§‹ç»ˆä¿å­˜ last.ckpt ç”¨äºç»­è®­
        auto_insert_metric_name=False,
    )
    callbacks.append(checkpoint_callback)

    # 2. LearningRateMonitor - æŒ‰ step è®°å½•å­¦ä¹ ç‡
    lr_monitor = LearningRateMonitor(logging_interval="step")
    callbacks.append(lr_monitor)

    # 3. RichProgressBar - ç¾åŒ–è¿›åº¦æ¡ (å¯é€‰)
    try:
        callbacks.append(RichProgressBar())
    except Exception:
        pass  # rich æœªå®‰è£…

    # 4. Curriculum Learning Callback (ä»… dense é˜¶æ®µä¸”å¯ç”¨æ—¶)
    curriculum_config = training_config.get("curriculum", {})
    if curriculum_config.get("enabled", False) and stage_name == "dense":
        curriculum_callback = CurriculumCallback(
            start_stage=curriculum_config.get("start_stage", 0),
            end_stage=curriculum_config.get("end_stage", 9),
            epochs_per_stage=curriculum_config.get("epochs_per_stage", 10),
        )
        callbacks.append(curriculum_callback)
        print(
            f"ğŸ“ˆ Curriculum Learning enabled: "
            f"stage {curriculum_config.get('start_stage', 0)} -> "
            f"{curriculum_config.get('end_stage', 9)}, "
            f"{curriculum_config.get('epochs_per_stage', 10)} epochs/stage"
        )

    # 5. Visualization Callback - è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”å›¾å¹¶è®°å½•åˆ° TensorBoard
    # å¯è§†åŒ–é…ç½®
    vis_config = training_config.get("visualization", {})
    if vis_config.get("enabled", True):
        vis_callback = VisualizationCallback(
            num_samples=vis_config.get("num_samples", 4),
            log_metrics=vis_config.get("log_metrics", True),
            prefix="Validation" if stage_name == "dense" else "Train",
        )
        callbacks.append(vis_callback)
        print(
            f"ğŸ¨ Visualization enabled: "
            f"{vis_config.get('num_samples', 4)} samples per validation"
        )

    # =========================================================================
    # Trainer é…ç½®
    # =========================================================================
    accelerator = "auto"
    devices = "auto"

    # è®¾å¤‡é…ç½®
    device_type = device_config.get("type")
    if device_type:
        if device_type == "cuda":
            accelerator = "gpu"
        elif device_type == "cpu":
            accelerator = "cpu"
        elif device_type == "xpu":
            accelerator = "xpu"

    # ç²¾åº¦é…ç½®
    precision = "16-mixed" if accelerator in ["gpu", "cuda"] else "32"

    trainer = pl.Trainer(
        # åŸºç¡€é…ç½®
        max_epochs=training_config.get("epochs", 50),
        accelerator=accelerator,
        devices=devices,
        precision=precision,
        # æ ¸å¿ƒä¿®å¤: é™åˆ¶æ¯ä¸ª epoch çš„ batch æ•°é‡
        limit_train_batches=limit_train_batches,
        limit_val_batches=limit_train_batches // 10,  # éªŒè¯æ›´å°‘
        # æ¢¯åº¦è£å‰ª
        gradient_clip_val=training_config.get("grad_clip", 1.0),
        # Callbacks & Logger
        callbacks=callbacks,
        logger=logger,
        # æ—¥å¿—é¢‘ç‡
        log_every_n_steps=logging_config.get("log_interval", 10),
        # éªŒè¯é¢‘ç‡
        val_check_interval=logging_config.get("vis_interval", 2),
        check_val_every_n_epoch=logging_config.get("vis_interval", 2),
        # æ€§èƒ½ä¼˜åŒ–
        enable_model_summary=True,
        enable_progress_bar=True,
        # æ–­ç‚¹ç»­è®­
        # æ³¨æ„: ckpt_path åœ¨ trainer.fit() ä¸­ä¼ å…¥ï¼Œè€Œéè¿™é‡Œ
    )

    return trainer


# =============================================================================
# è®­ç»ƒå‡½æ•°
# =============================================================================


def run_stage(
    config: dict,
    stage_name: str,
    init_from: Optional[str] = None,
    resume_from: Optional[str] = None,
) -> str:
    """
    è¿è¡Œå•ä¸ªè®­ç»ƒé˜¶æ®µ

    Args:
        config: å…¨å±€é…ç½®
        stage_name: é˜¶æ®µåç§° ("structural" æˆ– "dense")
        init_from: åˆå§‹åŒ–æƒé‡è·¯å¾„ (ç”¨äºè¿ç§»å­¦ä¹ )
        resume_from: æ–­ç‚¹ç»­è®­ checkpoint è·¯å¾„

    Returns:
        æœ€ä½³ checkpoint è·¯å¾„
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸš€ Starting stage: {stage_name}")
    print(f"{'=' * 60}\n")

    # è·å–é˜¶æ®µé…ç½®
    stage_config = get_stage_config(config, stage_name)
    training_config = stage_config.get("training", {})
    model_config = stage_config.get("model", {})
    data_config = stage_config.get("data", {})

    # =========================================================================
    # åˆ›å»º DataModule
    # =========================================================================
    datamodule = InkTraceDataModule(
        img_size=data_config.get("img_size", 64),
        batch_size=training_config.get("batch_size", 128),
        epoch_length=training_config.get("epoch_length", 10000),
        curriculum_stage=data_config.get("curriculum_stage", 0),
        num_workers=data_config.get("num_workers", 4),
        rust_threads=data_config.get("rust_threads", None),
    )

    # =========================================================================
    # åˆ›å»ºæ¨¡å‹
    # =========================================================================
    loss_weights = training_config.get("loss_weights", None)

    model = UnifiedTask(
        stage=stage_name,
        embed_dim=model_config.get("embed_dim", 128),
        num_layers=model_config.get("num_layers", 4),
        lr=float(training_config.get("lr", 1e-3)),
        weight_decay=float(training_config.get("weight_decay", 1e-4)),
        loss_weights=loss_weights,
        mask_ratio=float(training_config.get("mask_ratio", 0.6)),
        mask_strategy=training_config.get("mask_strategy", "block"),
        grad_clip=float(training_config.get("grad_clip", 1.0)),
    )

    # ä» checkpoint åˆå§‹åŒ–æƒé‡ (è¿ç§»å­¦ä¹ )
    if init_from and not resume_from:
        model.load_pretrained_weights(init_from, strict=False)

    # =========================================================================
    # åˆ›å»º Trainer
    # =========================================================================
    trainer = create_trainer(stage_config, stage_name, resume_from)

    # =========================================================================
    # å¼€å§‹è®­ç»ƒ
    # =========================================================================
    trainer.fit(
        model,
        datamodule=datamodule,
        ckpt_path=resume_from,  # æ–­ç‚¹ç»­è®­
    )

    # è¿”å›æœ€ä½³ checkpoint è·¯å¾„
    best_ckpt = trainer.checkpoint_callback.best_model_path
    print(f"\nâœ… Stage {stage_name} completed!")
    print(f"   Best checkpoint: {best_ckpt}")

    return best_ckpt


def run_all_stages(config: dict):
    """
    è¿è¡Œæ‰€æœ‰è®­ç»ƒé˜¶æ®µ

    æŒ‰é…ç½®æ–‡ä»¶ä¸­çš„ stages é¡ºåºæ‰§è¡Œ
    """
    stages = config.get("stages", [])
    if not stages:
        raise ValueError("No stages defined in config")

    last_ckpt = None

    for stage_info in stages:
        stage_name = stage_info["name"]

        # ç¡®å®š init_from
        init_from = stage_info.get("init_from")
        if init_from and last_ckpt and "*" in init_from:
            # æ›¿æ¢é€šé…ç¬¦
            init_from = init_from.replace("*", str(last_ckpt))
        elif not init_from and last_ckpt:
            # ä½¿ç”¨ä¸Šä¸€é˜¶æ®µçš„ checkpoint
            init_from = str(last_ckpt)

        # è¿è¡Œé˜¶æ®µ
        best_ckpt = run_stage(config, stage_name, init_from=init_from)
        last_ckpt = best_ckpt

    print(f"\n{'#' * 60}")
    print("ğŸ‰ All stages completed!")
    print(f"   Final checkpoint: {last_ckpt}")
    print(f"{'#' * 60}\n")


# =============================================================================
# CLI
# =============================================================================


def parse_args():
    parser = argparse.ArgumentParser(
        description="InkTrace PyTorch Lightning Training",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # é…ç½®
    parser.add_argument(
        "--config",
        type=str,
        default="configs/default.yaml",
        help="YAML é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--stage",
        type=str,
        choices=["structural", "dense"],
        help="è®­ç»ƒé˜¶æ®µ",
    )
    parser.add_argument(
        "--run-all-stages",
        action="store_true",
        help="è‡ªåŠ¨è¿è¡Œæ‰€æœ‰é˜¶æ®µ",
    )

    # Checkpoint
    parser.add_argument(
        "--init_from",
        type=str,
        help="ä» checkpoint åˆå§‹åŒ–æ¨¡å‹ (è¿ç§»å­¦ä¹ )",
    )
    parser.add_argument(
        "--resume",
        type=str,
        help="ä» checkpoint æ–­ç‚¹ç»­è®­",
    )

    # è¦†ç›–é…ç½® (å¯é€‰)
    parser.add_argument("--lr", type=float, help="è¦†ç›–å­¦ä¹ ç‡")
    parser.add_argument("--epochs", type=int, help="è¦†ç›–è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, help="è¦†ç›–æ‰¹æ¬¡å¤§å°")

    return parser.parse_args()


def main():
    args = parse_args()

    # åŠ è½½é…ç½®
    config = load_config(args.config)

    # è¦†ç›–é…ç½®
    if args.lr:
        config["training"]["lr"] = args.lr
    if args.epochs:
        config["training"]["epochs"] = args.epochs
    if args.batch_size:
        config["training"]["batch_size"] = args.batch_size

    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(114514, workers=True)

    # è¿è¡Œè®­ç»ƒ
    if args.run_all_stages:
        run_all_stages(config)
    elif args.stage:
        run_stage(
            config,
            args.stage,
            init_from=args.init_from,
            resume_from=args.resume,
        )
    else:
        raise ValueError("è¯·æŒ‡å®š --stage æˆ– --run-all-stages")


if __name__ == "__main__":
    main()
