# =============================================================================
# InkTrace V5 è®­ç»ƒé…ç½® (å®Œå…¨é‡æ„ç‰ˆ)
# =============================================================================
# è®¾è®¡åŸåˆ™ï¼š
#   1. å…¨å±€é»˜è®¤å€¼ + é˜¶æ®µè¦†ç›–é…ç½®
#   2. æ‰€æœ‰å‚æ•°ä¸ä»£ç é»˜è®¤å€¼ä¸¥æ ¼ä¸€è‡´
#   3. ç§»é™¤æœªä½¿ç”¨çš„å‚æ•°
#   4. æ·»åŠ ç¼ºå¤±çš„é‡è¦é…ç½®
#   5. å‚æ•°åˆ†ç±»æ¸…æ™°ï¼ˆmodel/training/data/logging/deviceï¼‰
#
# ä½¿ç”¨æ–¹æ³•ï¼š
#   python train_pl.py --config configs/default.yaml --stage structural
#   python train_pl.py --config configs/default.yaml --stage dense
#   python train_pl.py --config configs/default.yaml --run-all-stages
# =============================================================================


# =============================================================================
# å…¨å±€é»˜è®¤é…ç½® (æ‰€æœ‰é˜¶æ®µçš„åŸºç¡€)
# =============================================================================

model:
  # ----------------------------------------------------------------------
  # Encoder é…ç½® (StrokeEncoder)
  # ----------------------------------------------------------------------
  embed_dim: 192          # Transformer embedding ç»´åº¦ (æ›´å®½çš„è¡¨è¾¾)
  num_layers: 4           # Transformer å±‚æ•° (æ›´å¿«çš„è®­ç»ƒ)
  num_heads: 6            # Attention å¤´æ•° (192/6=32, æ ‡å‡†head_dim)
  dropout: 0.1            # Dropout ç‡

  # ----------------------------------------------------------------------
  # Decoder é…ç½® (UniversalDecoder)
  # ----------------------------------------------------------------------
  decoder_heads: 64       # Decoder head channels (å›ºå®šå€¼)
  decoder_kernel: 7       # NeXtBlock kernel size (å›ºå®šå€¼)

  # ----------------------------------------------------------------------
  # è®­ç»ƒæ¨¡å¼
  # ----------------------------------------------------------------------
  full_heads: true        # æ˜¯å¦è¾“å‡ºå…¨éƒ¨ 5 ä¸ªé¢„æµ‹å¤´

  # ----------------------------------------------------------------------
  # Structural é¢„è®­ç»ƒé…ç½®
  # ----------------------------------------------------------------------
  mask_ratio: 0.6         # é®æŒ¡æ¯”ä¾‹ (0.0-1.0)
  mask_strategy: "block"  # é®æŒ¡ç­–ç•¥: "block" | "random"
  mask_block_size: 8      # block ç­–ç•¥æ—¶çš„å—å¤§å° (åƒç´ )

training:
  # ----------------------------------------------------------------------
  # åŸºç¡€è®­ç»ƒå‚æ•°
  # ----------------------------------------------------------------------
  lr: 1e-3                # åˆå§‹å­¦ä¹ ç‡
  batch_size: 256         # ğŸ”¥ æ‰¹æ¬¡å¤§å° (ä¼˜åŒ–ï¼š5090 å¤§æ˜¾å­˜)
  epochs: 50              # è®­ç»ƒè½®æ•°
  epoch_length: 50000     # ğŸ”¥ æ¯ä¸ª epoch çš„æ ·æœ¬æ•° (å¢åŠ æ•°æ®é‡)

  # ä¼˜åŒ–å™¨å‚æ•°
  weight_decay: 1e-4      # AdamW æƒé‡è¡°å‡
  grad_clip: 1.0          # æ¢¯åº¦è£å‰ªé˜ˆå€¼ (max norm)

  # ----------------------------------------------------------------------
  # å­¦ä¹ ç‡è°ƒåº¦å™¨
  # ----------------------------------------------------------------------
  scheduler:
    type: "onecycle"      # è°ƒåº¦å™¨ç±»å‹: "onecycle" | "cosine" | "constant"
    warmup_epochs: 3      # é¢„çƒ­è½®æ•° (ç¨é•¿)
    pct_start: 0.1        # OneCycleLR: warmup å æ€»æ­¥æ•°æ¯”ä¾‹

  # ----------------------------------------------------------------------
  # Checkpoint ç®¡ç†
  # ----------------------------------------------------------------------
  checkpoint:
    save_dir: "checkpoints"
    keep_top_k: 5         # ä¿ç•™ top-k æœ€ä¼˜ checkpoint
    save_last: true       # å§‹ç»ˆä¿å­˜ last.ckpt
    monitor: "val/loss"   # ç›‘æ§æŒ‡æ ‡
    mode: "min"           # ä¼˜åŒ–æ–¹å‘: "min" | "max"

  # ----------------------------------------------------------------------
  # Loss æƒé‡ (Dense é˜¶æ®µä½¿ç”¨)
  # ----------------------------------------------------------------------
  loss_weights:
    skeleton: 10.0        # éª¨æ¶ (æœ€é‡è¦)
    keypoints: 5.0        # å…³é”®ç‚¹ (æ‹“æ‰‘èŠ‚ç‚¹)
    tangent: 2.0          # åˆ‡å‘åœº (å¯¹æ‹Ÿåˆé‡è¦)
    width: 1.0            # å®½åº¦
    offset: 1.0           # äºšåƒç´ åç§»

  # ----------------------------------------------------------------------
  # Curriculum Learning (æ¸è¿›å¼è®­ç»ƒ)
  # ----------------------------------------------------------------------
  curriculum:
    enabled: false        # é»˜è®¤å…³é—­ï¼Œåœ¨å…·ä½“é˜¶æ®µå¼€å¯
    start_stage: 0        # èµ·å§‹é˜¶æ®µ (0-9)
    end_stage: 6          # ç»“æŸé˜¶æ®µ (å»ºè®® 6)
    epochs_per_stage: 15  # ğŸ”¥ æ¯ä¸ªé˜¶æ®µè®­ç»ƒè½®æ•° (æ¯é˜¶æ®µå­¦ä¹…ä¸€ç‚¹)

  # ----------------------------------------------------------------------
  # å¯è§†åŒ–é…ç½®
  # ----------------------------------------------------------------------
  visualization:
    enabled: true
    num_samples: 8        # ğŸ”¥ æ›´å¤šå¯è§†åŒ–æ ·æœ¬
    log_interval: 1       # æ¯ N epoch å¯è§†åŒ–ä¸€æ¬¡
    log_metrics: true     # è®°å½• IoU/Precision/Recall

data:
  # ----------------------------------------------------------------------
  # æ•°æ®å¢å¼º
  # ----------------------------------------------------------------------
  img_size: 64            # å›¾åƒå°ºå¯¸ (æ–¹å½¢)

  # ----------------------------------------------------------------------
  # DataLoader é…ç½®
  # ----------------------------------------------------------------------
  batch_size: 256         # ğŸ”¥ æ‰¹æ¬¡å¤§å° (ä¸ training.batch_size ä¿æŒä¸€è‡´)
  num_workers: 14         # ğŸ”¥ DataLoader worker æ•°é‡ (16C ä¼˜åŒ–)
  pin_memory: true        # æ˜¯å¦ä½¿ç”¨ pin_memory (åŠ é€Ÿ GPU ä¼ è¾“)
  persistent_workers: true # ä¿æŒ worker è¿›ç¨‹ (å‡å°‘å¯åŠ¨å¼€é”€)

  # ----------------------------------------------------------------------
  # æ•°æ®ç”Ÿæˆé…ç½®
  # ----------------------------------------------------------------------
  rust_threads: 16        # ğŸ”¥ Rust ç”Ÿæˆå™¨çº¿ç¨‹æ•° (CPU æ»¡è½½)
  curriculum_stage: 0     # åˆå§‹ curriculum é˜¶æ®µ (0-9)

  # ----------------------------------------------------------------------
  # Ground Truth ç”Ÿæˆ
  # ----------------------------------------------------------------------
  keypoint_sigma: 2.0     # ğŸ”¥ é«˜æ–¯çƒ­åŠ›å›¾æ ‡å‡†å·® (æ›´å¹³æ»‘ï¼ŒæŠ—å™ªå£°)

logging:
  log_interval: 20        # ğŸ”¥ æ¯ N step è®°å½•ä¸€æ¬¡ (batch å˜å¤§ï¼Œé¢‘ç‡é™ä½)
  tensorboard_dir: "runs" # TensorBoard æ—¥å¿—ç›®å½•

device:
  accelerator: "gpu"      # ğŸ”¥ åŠ é€Ÿå™¨: å¼ºåˆ¶ GPU
  precision: "bf16-mixed" # ğŸ”¥ ç²¾åº¦: BF16 (5090 æœ€ä½³)


# =============================================================================
# å¤šé˜¶æ®µè®­ç»ƒæµæ°´çº¿ (Pipeline)
# =============================================================================

pipeline:
  # è®­ç»ƒé¡ºåº (æŒ‰åˆ—è¡¨é¡ºåºæ‰§è¡Œ)
  order:
    - "structural"
    - "dense"

  # è‡ªåŠ¨æƒé‡ä¼ é€’
  auto_transfer: true     # å°†ä¸Šä¸€é˜¶æ®µçš„æœ€ä¼˜æƒé‡ä¼ é€’ç»™ä¸‹ä¸€é˜¶æ®µ


# =============================================================================
# é˜¶æ®µå®šä¹‰ (è¦†ç›–é…ç½®)
# =============================================================================
# Curriculum Stages è¯´æ˜:
#   Stage 0:   å•ç¬”ç”» (æœ€ç®€å•)
#   Stage 1-3: å¤šç‹¬ç«‹ç¬”ç”» (é€’å¢: 1-3, 2-5, 3-8 ç¬”ç”»)
#   Stage 4-6: å¤šæ®µè¿ç»­ç¬”ç”» (é€’å¢: 2-3, 3-5, 4-8 æ®µ)
#   Stage 7-9: æ··åˆæ¨¡å¼ (å¤šæ¡å¤šæ®µè·¯å¾„, æœ€å¤æ‚)
# =============================================================================

stages:
  # =========================================================================
  # Phase 1: Structural Pretraining (ç»“æ„é¢„è®­ç»ƒ)
  # =========================================================================
  # ç›®æ ‡ï¼šè®© Encoder å­¦ä¼šä»æ®‹ç¼ºè¾“å…¥æ¨æ–­å®Œæ•´ç»“æ„
  # æ–¹æ³•ï¼šMasking + Reconstruction (ç±»ä¼¼ MAE)
  # ç‰¹ç‚¹ï¼šå…³é—­è·³è¿ï¼Œå¼ºè¿« Encoder åœ¨ bottleneck ç¼–ç å®Œæ•´ä¿¡æ¯
  # =========================================================================
  structural:
    description: "ç»“æ„é¢„è®­ç»ƒï¼šé®æŒ¡é‡å»ºä»»åŠ¡"

    model:
      full_heads: false   # åªè¾“å‡º skeleton + tangent
      mask_ratio: 0.6     # 60% é®æŒ¡
      mask_strategy: "block"

    training:
      lr: 1e-3
      epochs: 40          # ğŸ”¥ å¢åŠ è½®æ•°é€‚åº”å¤§æ•°æ®
      epoch_length: 40000 # ğŸ”¥ 4x æ•°æ®é‡ (1.6M total)
      batch_size: 256

      scheduler:
        type: "onecycle"
        warmup_epochs: 5
        pct_start: 0.1

      checkpoint:
        save_dir: "checkpoints/structural"
        monitor: "train/loss"  # é¢„è®­ç»ƒæ— éªŒè¯é›†
        keep_top_k: 3

      # Loss æƒé‡ (ä»… skeleton + tangent)
      loss_weights:
        skeleton: 1.0
        tangent: 1.0

      curriculum:
        enabled: false    # é¢„è®­ç»ƒä¸ç”¨ curriculum

      visualization:
        enabled: true
        num_samples: 8
        log_interval: 2

    data:
      curriculum_stage: 2  # ä¸­ç­‰å¤æ‚åº¦é¢„è®­ç»ƒ
      num_workers: 14
      rust_threads: 16

  # =========================================================================
  # Phase 2: Dense Prediction Training (å¯†é›†é¢„æµ‹è®­ç»ƒ)
  # =========================================================================
  # ç›®æ ‡ï¼šè®­ç»ƒå®Œæ•´çš„ 5-head å¯†é›†é¢„æµ‹
  # æ–¹æ³•ï¼šå¤šä»»åŠ¡å­¦ä¹  (Skeleton + Keypoints + Tangent + Width + Offset)
  # ç‰¹ç‚¹ï¼šä» structural checkpoint åˆå§‹åŒ–ï¼Œä½¿ç”¨ curriculum learning
  # =========================================================================
  dense:
    description: "å¯†é›†é¢„æµ‹è®­ç»ƒï¼šå¤šä»»åŠ¡å­¦ä¹ "

    # æƒé‡åˆå§‹åŒ–
    init_from: "auto"     # "auto" = ä½¿ç”¨ä¸Šä¸€é˜¶æ®µæœ€ä¼˜; æˆ–æŒ‡å®šè·¯å¾„
    freeze_encoder: false # æ˜¯å¦å†»ç»“ Encoder (å¯ç”¨äºè¿ç§»å­¦ä¹ )

    model:
      full_heads: true    # è¾“å‡ºå…¨éƒ¨ 5 ä¸ªå¤´

    training:
      lr: 5e-4            # æ¯”é¢„è®­ç»ƒä½ï¼Œæ›´ç¨³å®š
      epochs: 90         # ğŸ”¥ å¢åŠ è½®æ•°
      epoch_length: 60000 # ğŸ”¥ 6x æ•°æ®é‡ (6M total)
      batch_size: 256
      grad_clip: 2.0

      scheduler:
        type: "onecycle"
        warmup_epochs: 5
        pct_start: 0.05   # æ›´çŸ­çš„ warmup

      checkpoint:
        save_dir: "checkpoints/dense"
        monitor: "val/loss"
        keep_top_k: 5

      # Loss æƒé‡ (ç²¾è°ƒ)
      loss_weights:
        skeleton: 10.0    # æœ€é‡è¦
        keypoints: 5.0    # å…³é”®ç‚¹
        tangent: 2.0      # å¯¹æ›²çº¿æ‹Ÿåˆé‡è¦
        width: 1.0
        offset: 1.0

      # Curriculum Learning
      curriculum:
        enabled: true
        start_stage: 0
        end_stage: 6
        epochs_per_stage: 15 # ğŸ”¥ æ¯é˜¶æ®µè®­ç»ƒä¹…ä¸€ç‚¹

      visualization:
        enabled: true
        num_samples: 8
        log_interval: 1
        log_metrics: true

    data:
      curriculum_stage: 0  # ä»ç®€å•å¼€å§‹
      num_workers: 14
      rust_threads: 16

  # =========================================================================
  # Phase 3: End-to-End Finetuning (ç«¯åˆ°ç«¯å¾®è°ƒï¼Œå¯é€‰)
  # =========================================================================
  finetune:
    description: "ç«¯åˆ°ç«¯å¾®è°ƒï¼ˆå¯é€‰ï¼‰"

    init_from: "auto"
    freeze_encoder: false

    model:
      full_heads: true

    training:
      lr: 1e-4            # æ›´ä½å­¦ä¹ ç‡
      epochs: 20
      epoch_length: 50000
      batch_size: 256

      scheduler:
        type: "cosine"
        warmup_epochs: 1

      checkpoint:
        save_dir: "checkpoints/finetune"
        monitor: "val/loss"
        keep_top_k: 3

      loss_weights:
        skeleton: 10.0
        keypoints: 5.0
        tangent: 2.0
        width: 1.0
        offset: 1.0

      curriculum:
        enabled: false    # å¾®è°ƒé˜¶æ®µä¸ç”¨ curriculum

      visualization:
        enabled: true
        num_samples: 8

    data:
      curriculum_stage: 6  # ç›´æ¥ä½¿ç”¨å¤æ‚æ•°æ®

  # =========================================================================
  # Debug: å¿«é€Ÿè°ƒè¯•é…ç½®
  # =========================================================================
  debug:
    description: "å¿«é€Ÿè°ƒè¯•é…ç½®"

    model:
      full_heads: true

    training:
      lr: 1e-3
      epochs: 3
      epoch_length: 320   # å¾ˆå°ï¼Œå¿«é€Ÿè¿­ä»£
      batch_size: 32

      checkpoint:
        save_dir: "checkpoints/debug"
        keep_top_k: 1

      curriculum:
        enabled: false

      visualization:
        enabled: true
        num_samples: 2
        log_interval: 1

    data:
      num_workers: 2
      curriculum_stage: 0
