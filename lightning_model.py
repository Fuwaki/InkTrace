"""
PyTorch Lightning Module for InkTrace

æ”¯æŒä¸¤ç§è®­ç»ƒé˜¶æ®µï¼š
- structural: ç»“æ„é¢„è®­ç»ƒ (é®æŒ¡é‡å»º)
- dense: å¯†é›†é¢„æµ‹è®­ç»ƒ (å¤šä»»åŠ¡å­¦ä¹ )

æ ¸å¿ƒæ”¹è¿›ï¼š
- ä½¿ç”¨ self.trainer.estimated_stepping_batches è‡ªåŠ¨è®¡ç®— OneCycleLR æ­¥æ•°
- æ”¯æŒå¤šç§å­¦ä¹ ç‡è°ƒåº¦å™¨ (OneCycleLR, CosineAnnealingLR, Constant)
- è‡ªåŠ¨å¤„ç† Curriculum Learning
- ç®€åŒ–çš„æƒé‡åŠ è½½å’Œè¿ç§»å­¦ä¹ 
"""

import torch
import torch.optim as optim
import pytorch_lightning as pl
from typing import Dict, Optional, Literal

from models import ModelFactory, MaskingGenerator, StructuralPretrainLoss
from losses import DenseLoss
from vis_core import compute_metrics


class UnifiedTask(pl.LightningModule):
    """
    ç»Ÿä¸€çš„ Lightning Moduleï¼Œæ”¯æŒ structural å’Œ dense ä¸¤ç§è®­ç»ƒé˜¶æ®µ

    Args:
        stage: è®­ç»ƒé˜¶æ®µ ("structural" æˆ– "dense")
        embed_dim: Encoder embedding ç»´åº¦
        num_layers: Transformer å±‚æ•°
        lr: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        loss_weights: Dense Loss æƒé‡é…ç½® (ä»… dense é˜¶æ®µ)
        mask_ratio: é®æŒ¡æ¯”ä¾‹ (ä»… structural é˜¶æ®µ)
        mask_strategy: é®æŒ¡ç­–ç•¥ (ä»… structural é˜¶æ®µ)
        grad_clip: æ¢¯åº¦è£å‰ªé˜ˆå€¼
        scheduler_type: å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ ("onecycle", "cosine", "constant")
        warmup_epochs: é¢„çƒ­è½®æ•°
        pct_start: OneCycleLR warmup å æ¯”
    """

    def __init__(
        self,
        stage: Literal["structural", "dense"] = "dense",
        embed_dim: int = 192,  # ä¸ configs/default.yaml ä¸€è‡´
        num_layers: int = 4,   # ä¸ configs/default.yaml ä¸€è‡´
        lr: float = 1e-3,
        weight_decay: float = 1e-4,
        loss_weights: Optional[Dict[str, float]] = None,
        mask_ratio: float = 0.6,
        mask_strategy: str = "block",
        grad_clip: float = 1.0,
        scheduler_type: str = "onecycle",
        warmup_epochs: int = 2,
        pct_start: float = 0.1,
    ):
        super().__init__()
        self.save_hyperparameters()

        self.stage = stage
        self.lr = lr
        self.weight_decay = weight_decay
        self.grad_clip = grad_clip
        self.scheduler_type = scheduler_type
        self.warmup_epochs = warmup_epochs
        self.pct_start = pct_start

        # åˆ›å»ºæ¨¡å‹
        full_heads = stage == "dense"
        self.model = ModelFactory.create_unified_model(
            embed_dim=embed_dim,
            num_layers=num_layers,
            full_heads=full_heads,
            device="cpu",  # Lightning ä¼šå¤„ç†è®¾å¤‡è¿ç§»
        )

        # æ ¹æ®é˜¶æ®µè®¾ç½® Loss å’Œè¾…åŠ©ç»„ä»¶
        if stage == "structural":
            self.mask_gen = MaskingGenerator(
                mask_ratio=mask_ratio,
                strategy=mask_strategy,
            )
            self.criterion = StructuralPretrainLoss()
        else:  # dense
            self.criterion = DenseLoss(weights=loss_weights)

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        return self.model(x)

    def training_step(self, batch, batch_idx):
        """å•æ­¥è®­ç»ƒ"""
        imgs, targets = batch

        if self.stage == "structural":
            return self._structural_step(imgs, targets)
        else:
            return self._dense_step(imgs, targets)

    def _structural_step(self, imgs, targets):
        """Structural é¢„è®­ç»ƒæ­¥éª¤"""
        gt_skel = targets["skeleton"]
        gt_tan = targets["tangent"]

        # ç”Ÿæˆé®æŒ¡
        masked_imgs, mask = self.mask_gen(imgs)

        # å‰å‘ä¼ æ’­
        outputs = self.model.pretrain_forward(masked_imgs)
        pred_skel = outputs["skeleton"]
        pred_tan = outputs["tangent"]

        # è®¡ç®—æŸå¤±
        losses = self.criterion(pred_skel, pred_tan, gt_skel, gt_tan, mask)

        # æ—¥å¿—è®°å½•
        self.log("train/loss", losses["total"], prog_bar=True)
        self.log("train/loss_skeleton", losses["loss_skeleton"])
        self.log("train/loss_tangent", losses["loss_tangent"])

        return losses["total"]

    def _dense_step(self, imgs, targets):
        """Dense è®­ç»ƒæ­¥éª¤"""
        # å‰å‘ä¼ æ’­
        outputs = self.model(imgs)
        losses = self.criterion(outputs, targets)

        # æ£€æŸ¥ NaN/Inf
        if torch.isnan(losses["total"]) or torch.isinf(losses["total"]):
            self.log("train/nan_count", 1.0)
            return None  # è·³è¿‡è¿™ä¸ª batch

        # æ—¥å¿—è®°å½•
        self.log("train/loss", losses["total"], prog_bar=True)
        self.log("train/loss_skel", losses["loss_skel"])
        self.log("train/loss_keys", losses["loss_keys"])
        self.log("train/loss_tan", losses["loss_tan"])
        self.log("train/loss_width", losses["loss_width"])
        self.log("train/loss_off", losses["loss_off"])

        return losses["total"]

    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤ï¼ˆå¯é€‰ï¼‰"""
        imgs, targets = batch

        if self.stage == "structural":
            masked_imgs, mask = self.mask_gen(imgs)
            outputs = self.model.pretrain_forward(masked_imgs)
            losses = self.criterion(
                outputs["skeleton"],
                outputs["tangent"],
                targets["skeleton"],
                targets["tangent"],
                mask,
            )
        else:
            outputs = self.model(imgs)
            losses = self.criterion(outputs, targets)

        # è®°å½•æ€» loss
        self.log("val/loss", losses["total"], prog_bar=True, sync_dist=True)

        # Dense é˜¶æ®µï¼šè®¡ç®—è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
        if self.stage == "dense":
            metrics = compute_metrics(outputs, targets)

            # è®°å½•åˆ° TensorBoard
            self.log("val/iou", metrics["skel_iou"], sync_dist=True)
            self.log("val/precision", metrics["skel_precision"], sync_dist=True)
            self.log("val/recall", metrics["skel_recall"], sync_dist=True)
            self.log("val/f1", metrics["skel_f1"], sync_dist=True)
            self.log("val/kp_topo_recall", metrics["kp_topo_recall"], sync_dist=True)
            self.log("val/kp_geo_recall", metrics["kp_geo_recall"], sync_dist=True)

        return losses["total"]

    def configure_optimizers(self):
        """
        é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨

        æ”¯æŒå¤šç§è°ƒåº¦å™¨ï¼š
        - onecycle: OneCycleLR (æ¨èï¼Œè®­ç»ƒæ•ˆæœæœ€å¥½)
        - cosine: CosineAnnealingLR (é€‚åˆå¾®è°ƒ)
        - constant: å›ºå®šå­¦ä¹ ç‡ (è°ƒè¯•ç”¨)

        å…³é”®ç‚¹ï¼šä½¿ç”¨ self.trainer.estimated_stepping_batches è‡ªåŠ¨è®¡ç®—æ€»æ­¥æ•°
        """
        optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=self.lr,
            weight_decay=self.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8,
        )

        # ä½¿ç”¨ Lightning å†…ç½®çš„æ­¥æ•°ä¼°è®¡
        total_steps = self.trainer.estimated_stepping_batches
        print(f"\nğŸ“Š Scheduler: {self.scheduler_type}")
        print(f"   Total steps: {total_steps}")
        print(f"   Learning rate: {self.lr}")

        if self.scheduler_type == "onecycle":
            scheduler = optim.lr_scheduler.OneCycleLR(
                optimizer,
                max_lr=self.lr,
                total_steps=total_steps,
                pct_start=self.pct_start,
                anneal_strategy="cos",
                div_factor=25.0,  # åˆå§‹ lr = max_lr / 25
                final_div_factor=1e4,  # æœ€ç»ˆ lr = max_lr / 1e4
            )
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "step",
                    "frequency": 1,
                },
            }

        elif self.scheduler_type == "cosine":
            # CosineAnnealingLR æŒ‰ epoch æ›´æ–°
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.trainer.max_epochs,
                eta_min=self.lr * 0.01,  # æœ€ç»ˆ lr = 1% of initial
            )
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "epoch",
                    "frequency": 1,
                },
            }

        else:  # constant
            # å›ºå®šå­¦ä¹ ç‡ï¼Œä¸ä½¿ç”¨è°ƒåº¦å™¨
            return optimizer

    def on_train_epoch_start(self):
        """Epoch å¼€å§‹æ—¶çš„å›è°ƒ"""
        # è®°å½•å½“å‰ curriculum stage (å¦‚æœ DataModule æ”¯æŒ)
        if hasattr(self.trainer, "datamodule") and hasattr(
            self.trainer.datamodule, "curriculum_stage"
        ):
            self.log(
                "curriculum/stage",
                float(self.trainer.datamodule.curriculum_stage),
            )

        # è®°å½•å½“å‰ epoch
        self.log("train/epoch", float(self.current_epoch))

    def on_train_batch_end(self, outputs, batch, batch_idx):
        """æ¯ä¸ª batch ç»“æŸæ—¶çš„å›è°ƒï¼Œç”¨äºç›‘æ§è®­ç»ƒå¥åº·åº¦"""
        # è®°å½•æ¢¯åº¦èŒƒæ•° (æ¯ 100 æ­¥)
        if batch_idx % 100 == 0 and outputs is not None:
            total_norm = 0.0
            for p in self.model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm**0.5
            self.log("train/grad_norm", total_norm)

    # =========================================================================
    # æƒé‡åŠ è½½å·¥å…·æ–¹æ³•
    # =========================================================================

    def load_pretrained_weights(
        self,
        checkpoint_path: str,
        strict: bool = False,
        freeze_encoder: bool = False,
    ):
        """
        ä» checkpoint åŠ è½½é¢„è®­ç»ƒæƒé‡

        Args:
            checkpoint_path: checkpoint æ–‡ä»¶è·¯å¾„
            strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é… (False å…è®¸éƒ¨åˆ†åŠ è½½ï¼Œé€‚åˆè¿ç§»å­¦ä¹ )
            freeze_encoder: æ˜¯å¦å†»ç»“ Encoder æƒé‡
        """
        print(f"ğŸ“¦ Loading weights from: {checkpoint_path}")

        ckpt = torch.load(checkpoint_path, map_location="cpu")

        # æ”¯æŒå¤šç§ checkpoint æ ¼å¼
        if "state_dict" in ckpt:
            # Lightning checkpoint
            state_dict = ckpt["state_dict"]
            # ç§»é™¤ "model." å‰ç¼€ (å¦‚æœå­˜åœ¨)
            state_dict = {
                k.replace("model.", ""): v
                for k, v in state_dict.items()
                if k.startswith("model.")
            }
        elif "model_state_dict" in ckpt:
            # æ—§ç‰ˆæ‰‹åŠ¨ checkpoint
            state_dict = ckpt["model_state_dict"]
        else:
            state_dict = ckpt

        # åŠ è½½æƒé‡
        incompatible = self.model.load_state_dict(state_dict, strict=strict)

        if incompatible.missing_keys:
            print(f"  âš ï¸ Missing keys: {len(incompatible.missing_keys)}")
            if len(incompatible.missing_keys) <= 10:
                for k in incompatible.missing_keys:
                    print(f"     - {k}")
        if incompatible.unexpected_keys:
            print(f"  âš ï¸ Unexpected keys: {len(incompatible.unexpected_keys)}")

        # å†»ç»“ Encoder
        if freeze_encoder:
            print("  ğŸ”’ Freezing encoder weights")
            for param in self.model.encoder.parameters():
                param.requires_grad = False

        print("  âœ… Weights loaded successfully")

    @classmethod
    def load_from_checkpoint_with_stage(
        cls,
        checkpoint_path: str,
        stage: Literal["structural", "dense"],
        strict: bool = False,
        **kwargs,
    ) -> "UnifiedTask":
        """
        ä» checkpoint åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒåˆ‡æ¢è®­ç»ƒé˜¶æ®µ

        ç”¨äºä» structural è¿ç§»åˆ° dense é˜¶æ®µ

        Args:
            checkpoint_path: checkpoint è·¯å¾„
            stage: ç›®æ ‡è®­ç»ƒé˜¶æ®µ
            strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…
            **kwargs: ä¼ é€’ç»™ __init__ çš„é¢å¤–å‚æ•°
        """
        # åŠ è½½ checkpoint è·å–è¶…å‚æ•°
        ckpt = torch.load(checkpoint_path, map_location="cpu")
        hparams = ckpt.get("hyper_parameters", {})

        # ä» checkpoint è·å–æ¨¡å‹é…ç½®ï¼Œä½†ä½¿ç”¨æ–°çš„ stage
        model_kwargs = {
            "embed_dim": hparams.get("embed_dim", 192),  # ä¸ configs/default.yaml ä¸€è‡´
            "num_layers": hparams.get("num_layers", 4),   # ä¸ configs/default.yaml ä¸€è‡´
            "stage": stage,  # ä½¿ç”¨æ–°çš„ stage
        }
        model_kwargs.update(kwargs)

        # åˆ›å»ºæ–°æ¨¡å‹
        model = cls(**model_kwargs)

        # åŠ è½½æƒé‡ (éä¸¥æ ¼æ¨¡å¼ï¼Œå› ä¸º head å¯èƒ½ä¸åŒ)
        model.load_pretrained_weights(checkpoint_path, strict=strict)

        return model


class CurriculumCallback(pl.Callback):
    """
    Curriculum Learning å›è°ƒ

    æ ¹æ® epoch è‡ªåŠ¨æ›´æ–°æ•°æ®é›†çš„ curriculum stage
    """

    def __init__(
        self,
        start_stage: int = 0,
        end_stage: int = 9,
        epochs_per_stage: int = 10,
    ):
        super().__init__()
        self.start_stage = start_stage
        self.end_stage = end_stage
        self.epochs_per_stage = epochs_per_stage
        self.current_stage = start_stage

    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
        """æ¯ä¸ª epoch å¼€å§‹æ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° curriculum"""
        epoch = trainer.current_epoch
        target_stage = self.start_stage + (epoch // self.epochs_per_stage)
        target_stage = min(target_stage, self.end_stage)

        if target_stage != self.current_stage:
            old_stage = self.current_stage
            self.current_stage = target_stage

            # æ›´æ–° DataModule çš„ curriculum
            if hasattr(trainer, "datamodule") and hasattr(
                trainer.datamodule, "set_curriculum"
            ):
                trainer.datamodule.set_curriculum(target_stage)
                print(f"\nğŸ“ˆ Curriculum Update: Stage {old_stage} -> {target_stage}")

            # è®°å½•åˆ°æ—¥å¿—
            pl_module.log("curriculum/stage", float(target_stage))
