# InkTrace V5 é…ç½®å‚æ•°å®Œå…¨å‚è€ƒ

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ `configs/default.yaml` ä¸­æ‰€æœ‰å‚æ•°çš„å«ä¹‰ã€é»˜è®¤å€¼å’Œä»£ç å¯¹åº”å…³ç³»ã€‚

---

## ğŸ“‹ **å‚æ•°åˆ†ç±»æ€»è§ˆ**

```
configs/default.yaml
â”œâ”€â”€ model/           # æ¨¡å‹æ¶æ„å‚æ•°
â”œâ”€â”€ training/        # è®­ç»ƒè¶…å‚æ•°
â”œâ”€â”€ data/            # æ•°æ®åŠ è½½å‚æ•°
â”œâ”€â”€ logging/         # æ—¥å¿—é…ç½®
â”œâ”€â”€ device/          # ç¡¬ä»¶é…ç½®
â”œâ”€â”€ pipeline/        # å¤šé˜¶æ®µè®­ç»ƒæµç¨‹
â””â”€â”€ stages/          # å„é˜¶æ®µè¦†ç›–é…ç½®
```

---

## 1ï¸âƒ£ **Model æ¨¡å‹å‚æ•°**

### Encoder é…ç½® (StrokeEncoder)

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `embed_dim` | int | 128 | encoder.py:45 | Transformer embedding ç»´åº¦ |
| `num_layers` | int | 6 | encoder.py:47 | Transformer Encoder å±‚æ•° |
| `num_heads` | int | 4 | encoder.py:46 | Attention å¤´æ•° (éœ€æ•´é™¤ embed_dim) |
| `dropout` | float | 0.1 | encoder.py:48 | Dropout ç‡ |

**é‡è¦æç¤º**ï¼š
- `embed_dim=128` å¯¹äº 5 ä¸ªå¯†é›†é¢„æµ‹å¤´å¯èƒ½ä¸è¶³ï¼Œå»ºè®®æ”¹ä¸º **256**
- `num_layers=6` æ˜¯åˆç†å€¼ï¼Œä¸è¦å‡å°‘
- `num_heads=4` é…åˆ `embed_dim=128`ï¼Œæ¯ä¸ª head çš„ dim=32

**å‚æ•°å…³ç³»**ï¼š
```
head_dim = embed_dim / num_heads
å¯¹äº embed_dim=128, num_heads=4: head_dim = 32 (æ ‡å‡†å€¼)
å¦‚æœ embed_dim=256, å»ºè®®æ”¹ä¸º num_heads=8: head_dim = 32
```

---

### Decoder é…ç½® (UniversalDecoder)

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `decoder_heads` | int | 64 | decoder.py:243 | Decoder head channels (å›ºå®š) |
| `decoder_kernel` | int | 7 | decoder.py:236 | NeXtBlock å·ç§¯æ ¸å¤§å° (å›ºå®š) |

**æ³¨æ„**ï¼šè¿™äº›å‚æ•°ç›®å‰ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ï¼Œyaml ä¸­å®šä¹‰ä»…ä¸ºæ–‡æ¡£ç›®çš„ã€‚

---

### è®­ç»ƒæ¨¡å¼

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `full_heads` | bool | true | models.py:179 | æ˜¯å¦è¾“å‡ºå…¨éƒ¨ 5 ä¸ªé¢„æµ‹å¤´ |

- `false`: åªè¾“å‡º skeleton + tangent (structural é˜¶æ®µ)
- `true`: è¾“å‡ºå…¨éƒ¨ 5 ä¸ªå¤´ (dense é˜¶æ®µ)

---

### Structural é¢„è®­ç»ƒé…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `mask_ratio` | float | 0.6 | models.py:32 | é®æŒ¡æ¯”ä¾‹ (0.0-1.0) |
| `mask_strategy` | str | "block" | models.py:37 | é®æŒ¡ç­–ç•¥: "block" \| "random" |
| `mask_block_size` | int | 8 | models.py:41 | block ç­–ç•¥çš„å—å¤§å° (åƒç´ ) |

**Mask ç­–ç•¥è¯´æ˜**ï¼š
- `"block"`: éšæœºé®æŒ¡è‹¥å¹²çŸ©å½¢å—ï¼ˆç±»ä¼¼ MAEï¼‰ï¼Œ**æ¨è**
- `"random"`: éšæœºåƒç´ é®æŒ¡

---

## 2ï¸âƒ£ **Training è®­ç»ƒå‚æ•°**

### åŸºç¡€è®­ç»ƒå‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `lr` | float | 1e-3 | lightning_model.py:49 | åˆå§‹å­¦ä¹ ç‡ |
| `batch_size` | int | 128 | lightning_model.py | æ‰¹æ¬¡å¤§å° |
| `epochs` | int | 50 | train_pl.py:404 | è®­ç»ƒè½®æ•° |
| `epoch_length` | int | 10000 | train_pl.py:303 | æ¯ä¸ª epoch çš„æ ·æœ¬æ•° |
| `weight_decay` | float | 1e-4 | lightning_model.py:50 | AdamW æƒé‡è¡°å‡ |
| `grad_clip` | float | 1.0 | lightning_model.py:54 | æ¢¯åº¦è£å‰ªé˜ˆå€¼ (max norm) |

**é‡è¦è®¡ç®—**ï¼š
```
batches_per_epoch = epoch_length / batch_size
ä¾‹å¦‚ï¼š10000 / 128 = 78 batches/epoch

total_batches = epochs * batches_per_epoch
ä¾‹å¦‚ï¼š50 * 78 = 3900 batches
```

---

### å­¦ä¹ ç‡è°ƒåº¦å™¨

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `scheduler.type` | str | "onecycle" | lightning_model.py:206 | è°ƒåº¦å™¨ç±»å‹ |
| `scheduler.warmup_epochs` | int | 2 | lightning_model.py:56 | é¢„çƒ­è½®æ•° |
| `scheduler.pct_start` | float | 0.1 | lightning_model.py:57 | OneCycleLR warmup å æ¯” |

**è°ƒåº¦å™¨ç±»å‹**ï¼š
- `"onecycle"`: OneCycleLR (æ¨èï¼Œè®­ç»ƒæ•ˆæœæœ€å¥½)
- `"cosine"`: CosineAnnealingLR (é€‚åˆå¾®è°ƒ)
- `"constant"`: å›ºå®šå­¦ä¹ ç‡ (è°ƒè¯•ç”¨)

**OneCycleLR å‚æ•°è®¡ç®—**ï¼ˆlightning_model.py:206-215ï¼‰ï¼š
```python
total_steps = trainer.estimated_stepping_batches  # è‡ªåŠ¨è®¡ç®—
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=lr,                    # æœ€é«˜å­¦ä¹ ç‡
    total_steps=total_steps,      # æ€»æ­¥æ•°
    pct_start=pct_start,          # warmup å æ¯”
    div_factor=25.0,              # åˆå§‹ lr = max_lr / 25
    final_div_factor=1e4,         # æœ€ç»ˆ lr = max_lr / 1e4
)
```

---

### Checkpoint ç®¡ç†

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `checkpoint.save_dir` | str | "checkpoints" | train_pl.py:321 | ä¿å­˜ç›®å½• |
| `checkpoint.keep_top_k` | int | 3 | train_pl.py:334 | ä¿ç•™ top-k æœ€ä¼˜ |
| `checkpoint.save_last` | bool | true | train_pl.py:337 | ä¿å­˜ last.ckpt |
| `checkpoint.monitor` | str | "val/loss" | train_pl.py:329 | ç›‘æ§æŒ‡æ ‡ |
| `checkpoint.mode` | str | "min" | train_pl.py:335 | "min" æˆ– "max" |

**ç›‘æ§æŒ‡æ ‡è¯´æ˜**ï¼š
- `"train/loss"`: è®­ç»ƒ loss (structural é˜¶æ®µï¼Œæ— éªŒè¯é›†)
- `"val/loss"`: éªŒè¯ loss (dense é˜¶æ®µ)

---

### Loss æƒé‡ (Dense é˜¶æ®µ)

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `loss_weights.skeleton` | float | 10.0 | losses.py | éª¨æ¶ loss æƒé‡ |
| `loss_weights.keypoints` | float | 5.0 | losses.py | å…³é”®ç‚¹ loss æƒé‡ |
| `loss_weights.tangent` | float | 2.0 | losses.py | åˆ‡å‘åœº loss æƒé‡ |
| `loss_weights.width` | float | 1.0 | losses.py | å®½åº¦ loss æƒé‡ |
| `loss_weights.offset` | float | 1.0 | losses.py | åç§» loss æƒé‡ |

**æ€» Loss è®¡ç®—**ï¼ˆlosses.pyï¼‰ï¼š
```python
L_total = 10.0 * L_skeleton + 5.0 * L_keypoints + 2.0 * L_tangent
          + 1.0 * L_width + 1.0 * L_offset
```

---

### Curriculum Learning

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `curriculum.enabled` | bool | false | train_pl.py:353 | æ˜¯å¦å¯ç”¨ |
| `curriculum.start_stage` | int | 0 | train_pl.py:377 | èµ·å§‹é˜¶æ®µ (0-9) |
| `curriculum.end_stage` | int | 6 | train_pl.py:378 | ç»“æŸé˜¶æ®µ (0-9) |
| `curriculum.epochs_per_stage` | int | 10 | train_pl.py:379 | æ¯é˜¶æ®µè½®æ•° |

**Curriculum Stages è¯´æ˜**ï¼š
- Stage 0: å•ç¬”ç”» (æœ€ç®€å•)
- Stage 1-3: å¤šç‹¬ç«‹ç¬”ç”» (é€’å¢: 1-3, 2-5, 3-8 ç¬”ç”»)
- Stage 4-6: å¤šæ®µè¿ç»­ç¬”ç”» (é€’å¢: 2-3, 3-5, 4-8 æ®µ)
- Stage 7-9: æ··åˆæ¨¡å¼ (å¤šæ¡å¤šæ®µè·¯å¾„, æœ€å¤æ‚)

**è‡ªåŠ¨å‡çº§æœºåˆ¶**ï¼ˆlightning_model.py:387-405ï¼‰ï¼š
```python
target_stage = start_stage + (current_epoch // epochs_per_stage)
target_stage = min(target_stage, end_stage)
```

---

### å¯è§†åŒ–é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `visualization.enabled` | bool | true | train_pl.py:369 | æ˜¯å¦å¯ç”¨ |
| `visualization.num_samples` | int | 4 | train_pl.py:371 | æ¯æ¬¡å¯è§†åŒ–æ ·æœ¬æ•° |
| `visualization.log_interval` | int | 1 | train_pl.py:373 | æ¯ N epoch å¯è§†åŒ– |
| `visualization.log_metrics` | bool | true | train_pl.py:372 | è®°å½• IoU/Precision/Recall |

---

## 3ï¸âƒ£ **Data æ•°æ®å‚æ•°**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `img_size` | int | 64 | train_pl.py:482 | å›¾åƒå°ºå¯¸ (æ–¹å½¢) |
| `batch_size` | int | 128 | train_pl.py:483 | æ‰¹æ¬¡å¤§å° (ä¸ training ä¿æŒä¸€è‡´) |
| `num_workers` | int | 8 | train_pl.py:486 | DataLoader worker æ•°é‡ |
| `pin_memory` | bool | true | train_pl.py:488 | æ˜¯å¦ä½¿ç”¨ pin_memory |
| `persistent_workers` | bool | true | train_pl.py:489 | ä¿æŒ worker è¿›ç¨‹ |
| `rust_threads` | int\|null | null | train_pl.py:487 | Rust ç”Ÿæˆå™¨çº¿ç¨‹æ•° (null=è‡ªåŠ¨) |
| `curriculum_stage` | int | 0 | train_pl.py:485 | åˆå§‹ curriculum é˜¶æ®µ |
| `keypoint_sigma` | float | 1.5 | train_pl.py:490 | é«˜æ–¯çƒ­åŠ›å›¾æ ‡å‡†å·® |

**é«˜æ–¯çƒ­åŠ›å›¾è¯´æ˜**ï¼ˆdense_heads.py:149-160ï¼‰ï¼š
```python
GT heatmap = exp(-((x-x0)^2 + (y-y0)^2) / (2 * sigma^2))
```
- `sigma=1.5`: æ ‡å‡†é…ç½®
- `sigma=2.0`: æ›´å¹³æ»‘çš„æŠ—å™ªå£°é…ç½®
- `sigma=1.0`: æ›´é”åˆ©çš„ç²¾ç¡®å®šä½

---

## 4ï¸âƒ£ **Logging & Device**

### Logging

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `logging.log_interval` | int | 10 | train_pl.py:417 | æ¯ N step è®°å½•ä¸€æ¬¡ |
| `logging.tensorboard_dir` | str | "runs" | train_pl.py:308 | TensorBoard ç›®å½• |

---

### Device

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `device.accelerator` | str | "auto" | train_pl.py:405 | åŠ é€Ÿå™¨ç±»å‹ |
| `device.precision` | str | "16-mixed" | train_pl.py:406 | ç²¾åº¦æ¨¡å¼ |

**åŠ é€Ÿå™¨ç±»å‹**ï¼š
- `"auto"`: è‡ªåŠ¨æ£€æµ‹ (æ¨è)
- `"gpu"`: NVIDIA GPU
- `"cpu"`: CPU (ç²¾åº¦è‡ªåŠ¨é™ä¸º 32)
- `"mps"`: Apple Silicon GPU

**ç²¾åº¦æ¨¡å¼**ï¼š
- `"32"`: 32 ä½æµ®ç‚¹ (FP32)
- `"16-mixed"`: æ··åˆç²¾åº¦ (FP16 + FP32)ï¼Œæ¨è
- `"bf16-mixed"`: BFloat16 æ··åˆç²¾åº¦ (æ–° GPU)

---

## 5ï¸âƒ£ **Pipeline å¤šé˜¶æ®µè®­ç»ƒ**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä»£ç ä½ç½® | è¯´æ˜ |
|------|------|--------|----------|------|
| `pipeline.order` | list | ["structural", "dense"] | train_pl.py:557 | è®­ç»ƒé¡ºåº |
| `pipeline.auto_transfer` | bool | true | train_pl.py:558 | è‡ªåŠ¨æƒé‡ä¼ é€’ |

**æƒé‡ä¼ é€’é€»è¾‘**ï¼ˆtrain_pl.py:577-595ï¼‰ï¼š
```python
for stage in pipeline.order:
    init_from = last_best_checkpoint  # ä¸Šä¸€é˜¶æ®µçš„æœ€ä¼˜ checkpoint
    best_ckpt = run_stage(config, stage, init_from=init_from)
    last_best_checkpoint = best_ckpt  # ä¼ é€’ç»™ä¸‹ä¸€é˜¶æ®µ
```

---

## 6ï¸âƒ£ **Stages é˜¶æ®µé…ç½®**

æ¯ä¸ªé˜¶æ®µå¯ä»¥è¦†ç›–å…¨å±€é»˜è®¤å€¼ï¼Œæ”¯æŒæ·±åº¦åˆå¹¶ã€‚

### é˜¶æ®µçº§ç‰¹æ®Šå‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `init_from` | str\|"auto" | åˆå§‹åŒ–æƒé‡è·¯å¾„ ("auto" = è‡ªåŠ¨ä¼ é€’) |
| `freeze_encoder` | bool | æ˜¯å¦å†»ç»“ Encoder æƒé‡ |
| `description` | str | é˜¶æ®µæè¿° |

---

## ğŸ”§ **å‚æ•°ä¼˜å…ˆçº§**

```
å‘½ä»¤è¡Œå‚æ•° > é˜¶æ®µé…ç½® > å…¨å±€é»˜è®¤å€¼
```

**ç¤ºä¾‹**ï¼ˆtrain_pl.py:685-690ï¼‰ï¼š
```python
# å‘½ä»¤è¡Œè¦†ç›–
python train_pl.py --lr 2e-4 --batch_size 64

# ç­‰ä»·äºä¿®æ”¹ yaml
training:
  lr: 2e-4
  batch_size: 64
```

---

## ğŸ“Š **æ¨èé…ç½®**

### ğŸ”´ **é«˜æ€§èƒ½é…ç½®** (GPUå……è¶³)

```yaml
model:
  embed_dim: 256          # æå‡ç‰¹å¾å®¹é‡
  num_heads: 8            # åŒ¹é… 256 ç»´

training:
  epoch_length: 80000     # 8x æ•°æ®é‡
  batch_size: 64          # ä¸ºæ›´å¤§ embed_dim è…¾ç©ºé—´

data:
  img_size: 128           # 4x åˆ†è¾¨ç‡
  keypoint_sigma: 2.0     # è¡¥å¿åˆ†è¾¨ç‡æå‡
```

**é¢„æœŸæ•ˆæœ**ï¼š
- å‚æ•°é‡: 700K â†’ 1.1M
- è®­ç»ƒæ—¶é—´: ~2x
- ç²¾åº¦: æ˜¾è‘—æå‡

---

### ğŸŸ¢ **è½»é‡é…ç½®** (CPUæ¨ç†)

```yaml
model:
  embed_dim: 128          # ä¿æŒè½»é‡
  num_layers: 6

training:
  epoch_length: 50000     # 5x æ•°æ®é‡
  epochs: 150             # 3x è®­ç»ƒè½®æ•°
  lr: 5e-4                # ä¿å®ˆå­¦ä¹ ç‡
```

**é¢„æœŸæ•ˆæœ**ï¼š
- å‚æ•°é‡: ~700K
- CPU æ¨ç†: ~10ms/image
- ç²¾åº¦: è‰¯å¥½ (é€šè¿‡æ›´é•¿è®­ç»ƒè¡¥å¿)

---

### ğŸŸ¡ **è°ƒè¯•é…ç½®**

```yaml
model:
  embed_dim: 64           # æå°æ¨¡å‹
  num_layers: 2

training:
  epoch_length: 1000      # å¿«é€Ÿè¿­ä»£
  epochs: 5
  batch_size: 32
```

---

## âš ï¸ **å¸¸è§é™·é˜±**

### 1. embed_dim ä¸ num_heads ä¸åŒ¹é…

âŒ **é”™è¯¯**ï¼š
```yaml
model:
  embed_dim: 128
  num_heads: 8    # 128 / 8 = 16 (å¤ªå°)
```

âœ… **æ­£ç¡®**ï¼š
```yaml
model:
  embed_dim: 128
  num_heads: 4     # 128 / 4 = 32 (æ ‡å‡†)
```

---

### 2. batch_size ä¸ epoch_length ä¸åŒ¹é…

âŒ **é”™è¯¯**ï¼š
```yaml
training:
  batch_size: 128
  epoch_length: 100   # åªæœ‰ 0.78 batch/epoch (å¤ªå°‘)
```

âœ… **æ­£ç¡®**ï¼š
```yaml
training:
  batch_size: 128
  epoch_length: 10000  # 78 batches/epoch (åˆç†)
```

---

### 3. mask_ratio æœªæ”¾åœ¨ model ä¸‹

âŒ **æ—§é…ç½®**ï¼š
```yaml
training:
  mask_ratio: 0.6   # ä½ç½®é”™è¯¯
```

âœ… **æ–°é…ç½®**ï¼š
```yaml
model:
  mask_ratio: 0.6   # æ­£ç¡®ä½ç½®
```

---

## ğŸ“Œ **æ€»ç»“**

### âœ… å·²ä¿®å¤çš„é—®é¢˜

1. âœ… ç»Ÿä¸€ `num_layers` é»˜è®¤å€¼ä¸º **6** (yaml + encoder.py + models.py)
2. âœ… ç»Ÿä¸€ `embed_dim` é»˜è®¤å€¼ä¸º **128** (yaml + encoder.py + models.py)
3. âœ… å°† `mask_ratio` å’Œ `mask_strategy` ç§»åˆ° `model` ä¸‹
4. âœ… ç§»é™¤æœªä½¿ç”¨çš„ `min_lr` å’Œ `warmup_start_lr`
5. âœ… æ·»åŠ  `mask_block_size` é…ç½®
6. âœ… æ·»åŠ  `decoder_heads` å’Œ `decoder_kernel` æ–‡æ¡£è¯´æ˜

### ğŸ¯ å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥è¡¨

| å‚æ•° | yaml | encoder.py | models.py | train_pl.py | çŠ¶æ€ |
|------|------|------------|-----------|-------------|------|
| embed_dim | 128 | 128 | 128 | 128 | âœ… |
| num_layers | 6 | 6 | 6 | 6 | âœ… |
| num_heads | 4 | 4 | 4 | 4 | âœ… |
| mask_ratio | model | - | 0.6 | model | âœ… |

### ğŸš€ ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®

1. **P0**: å°† `embed_dim` æå‡åˆ° **256**
2. **P0**: å°† `epoch_length` æå‡åˆ° **50k+**
3. **P1**: è€ƒè™‘æå‡ `img_size` åˆ° **128**
4. **P2**: æ·»åŠ æ›´å¤šå¯é…ç½®å‚æ•°ï¼ˆå¦‚ `stem_channels`ï¼‰
