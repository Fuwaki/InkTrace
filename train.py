#!/usr/bin/env python3
"""
ç»Ÿä¸€è®­ç»ƒè„šæœ¬ - æ›¿ä»£ train_structural.py å’Œ train_dense.py

ç‰¹æ€§:
  - å•ä¸€è„šæœ¬æ”¯æŒæ‰€æœ‰è®­ç»ƒæ¨¡å¼
  - YAML é…ç½®æ–‡ä»¶
  - è‡ªåŠ¨ checkpoint ç®¡ç†
  - å¤šé˜¶æ®µè®­ç»ƒ
  - ç®€å•çš„æ¢å¤è®­ç»ƒ

ä½¿ç”¨æ–¹æ³•:
    # å•é˜¶æ®µè®­ç»ƒï¼ˆstructural pretrainï¼‰
    python train.py --config configs/default.yaml --stage structural

    # å•é˜¶æ®µè®­ç»ƒï¼ˆdenseï¼‰
    python train.py --config configs/default.yaml --stage dense --init_from checkpoints/structural/checkpoint_best.pth

    # å¤šé˜¶æ®µè‡ªåŠ¨è®­ç»ƒ
    python train.py --config configs/default.yaml --run-all-stages

    # æ¢å¤è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹é…ç½®ï¼‰
    python train.py --resume checkpoints/structural/checkpoint_latest.pth
"""

import argparse
import math

import torch
import torch.optim as optim
from torch.utils.data import DataLoader

from models import ModelFactory, MaskingGenerator, StructuralPretrainLoss
from losses import DenseLoss
from datasets_v2 import DenseInkTraceDataset, collate_dense_batch
from train_lib import Config, BaseTrainer
from visualize_dense import DenseVisualizer


# ============================================================================
# Structural Pretraining Trainer
# ============================================================================


class StructuralTrainer(BaseTrainer):
    """ç»“æ„é¢„è®­ç»ƒè®­ç»ƒå™¨"""

    def __init__(self, config: Config, init_from: str = None):
        super().__init__(config, stage_name="structural")

        # æ¨¡å‹
        self.model = ModelFactory.create_unified_model(
            embed_dim=config.model["embed_dim"],
            num_layers=config.model["num_layers"],
            full_heads=False,
            device=self.device,
        )

        # ä» checkpoint åˆå§‹åŒ–
        if init_from:
            self.load_checkpoint(init_from, load_optimizer=False)

        # æ©ç ç”Ÿæˆå™¨ & æŸå¤±å‡½æ•°
        self.mask_gen = MaskingGenerator(
            mask_ratio=config.training.get("mask_ratio", 0.6),
            strategy=config.training.get("mask_strategy", "block"),
        )
        self.criterion = StructuralPretrainLoss()

        # ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
        lr = float(config.training["lr"])
        weight_decay = float(config.training.get("weight_decay", 1e-4))
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
        )
        steps_per_epoch = math.ceil(
            config.training["epoch_length"] / config.training["batch_size"]
        )
        total_steps = steps_per_epoch * config.training["epochs"]
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=lr,
            total_steps=total_steps,
        )

        print(f"\n{'=' * 60}")
        print(f"Structural Pretraining")
        print(
            f"  Model: embed_dim={config.model['embed_dim']}, num_layers={config.model['num_layers']}"
        )
        print(f"  Mask ratio: {config.training.get('mask_ratio', 0.6)}")
        print(
            f"  Training: lr={config.training['lr']}, epochs={config.training['epochs']}"
        )
        print(f"{'=' * 60}\n")

    def train_step(self, batch):
        # æ–°æ•°æ®é›†è¿”å› (imgs, targets) å…ƒç»„
        imgs, targets = batch
        imgs = imgs.to(self.device)
        gt_skel = targets["skeleton"].to(self.device)
        gt_tan = targets["tangent"].to(self.device)

        # ç”Ÿæˆæ©ç 
        masked_imgs, mask = self.mask_gen(imgs)
        mask = mask.to(self.device)

        # å‰å‘ä¼ æ’­
        self.optimizer.zero_grad()
        outputs = self.model.pretrain_forward(masked_imgs)
        pred_skel = outputs["skeleton"]
        pred_tan = outputs["tangent"]

        # æŸå¤±
        losses = self.criterion(pred_skel, pred_tan, gt_skel, gt_tan, mask)
        loss = losses["total"]

        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=self.config.training.get("grad_clip", 1.0),
        )
        self.optimizer.step()
        self.scheduler.step()

        return {
            "total": loss.item(),
            "skeleton": losses["loss_skeleton"].item(),
            "tangent": losses["loss_tangent"].item(),
        }


# ============================================================================
# Dense Training Trainer (with Curriculum Learning Support)
# ============================================================================


class DenseTrainer(BaseTrainer):
    """
    Dense è®­ç»ƒå™¨ - æ”¯æŒæ¸è¿›å¼è®­ç»ƒ (Curriculum Learning)

    Curriculum Stages (Stage 0-9):
      - Stage 0: å•ç¬”ç”»
      - Stage 1-3: å¤šç‹¬ç«‹ç¬”ç”»ï¼ˆé€’å¢: 1-3, 2-5, 3-8ï¼‰
      - Stage 4-6: å¤šæ®µè¿ç»­ç¬”ç”»ï¼ˆé€’å¢: 2-3, 3-5, 4-8ï¼‰
      - Stage 7-9: æ··åˆæ¨¡å¼ï¼ˆå¤šæ¡å¤šæ®µè·¯å¾„ï¼‰
    """

    def __init__(self, config: Config, init_from: str = None):
        super().__init__(config, stage_name="dense")

        # å½“å‰ curriculum é˜¶æ®µ
        self.curriculum_stage = config.data.get("curriculum_stage", 0)
        self.dataset = None  # Will be set later

        # Curriculum é…ç½®
        self.curriculum_config = config.training.get("curriculum", {})
        self.curriculum_enabled = self.curriculum_config.get("enabled", False)
        self.curriculum_epochs_per_stage = self.curriculum_config.get(
            "epochs_per_stage", 10
        )
        self.curriculum_start_stage = self.curriculum_config.get("start_stage", 0)
        self.curriculum_end_stage = self.curriculum_config.get("end_stage", 9)

        # æ¨¡å‹
        self.model = ModelFactory.create_unified_model(
            embed_dim=config.model.get("embed_dim", 128),
            num_layers=config.model.get("num_layers", 4),
            full_heads=True,
            device=self.device,
        )

        # ä» checkpoint åˆå§‹åŒ– (è¿ç§»å­¦ä¹ )
        if init_from:
            state = self.load_checkpoint(
                init_from, load_optimizer=False, strict=False, reset_epoch=True
            )
            # éªŒè¯é…ç½®åŒ¹é…
            ckpt_embed_dim = state["config"].model.get("embed_dim", 128)
            if ckpt_embed_dim != config.model.get("embed_dim", 128):
                print(f"  Warning: embed_dim mismatch! Checkpoint has {ckpt_embed_dim}")

        # æŸå¤±å‡½æ•°
        loss_weights = config.training.get("loss_weights", None)
        self.criterion = DenseLoss(weights=loss_weights)

        # ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
        lr = float(config.training["lr"])
        weight_decay = float(config.training.get("weight_decay", 1e-4))
        self.optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=lr,
            weight_decay=weight_decay,
        )
        steps_per_epoch = math.ceil(
            config.training["epoch_length"] / config.training["batch_size"]
        )
        total_steps = steps_per_epoch * config.training["epochs"]
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer, max_lr=lr, total_steps=total_steps
        )

        self._print_info()

    def _print_info(self):
        """æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯"""
        print(f"\n{'=' * 60}")
        print(f"Dense Training (Curriculum Learning)")
        print(
            f"  Model: embed_dim={self.config.model.get('embed_dim', 128)}, "
            f"num_layers={self.config.model.get('num_layers', 4)}"
        )
        print(
            f"  Training: lr={self.config.training['lr']}, "
            f"epochs={self.config.training['epochs']}"
        )
        print(f"  Initial Curriculum Stage: {self.curriculum_stage}")
        if self.curriculum_enabled:
            print(
                f"  Curriculum: ENABLED (stage {self.curriculum_start_stage} -> "
                f"{self.curriculum_end_stage}, {self.curriculum_epochs_per_stage} epochs/stage)"
            )
        else:
            print(f"  Curriculum: DISABLED (fixed stage)")
        print(f"{'=' * 60}\n")

    def set_dataset(self, dataset: DenseInkTraceDataset):
        """è®¾ç½®æ•°æ®é›†å¼•ç”¨ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´ curriculum"""
        self.dataset = dataset

    def update_curriculum(self, epoch: int):
        """
        æ ¹æ® epoch æ›´æ–° curriculum é˜¶æ®µ

        Returns:
            bool: æ˜¯å¦å‘ç”Ÿäº†é˜¶æ®µåˆ‡æ¢
        """
        if not self.curriculum_enabled or self.dataset is None:
            return False

        # è®¡ç®—å½“å‰åº”è¯¥å¤„äºå“ªä¸ªé˜¶æ®µ
        relative_epoch = epoch
        target_stage = self.curriculum_start_stage + (
            relative_epoch // self.curriculum_epochs_per_stage
        )
        target_stage = min(target_stage, self.curriculum_end_stage)

        if target_stage != self.curriculum_stage:
            old_stage = self.curriculum_stage
            self.curriculum_stage = target_stage
            self.dataset.set_curriculum(target_stage)
            print(f"\nğŸ“ˆ Curriculum Update: Stage {old_stage} -> {target_stage}")
            return True
        return False

    def train_step(self, batch):
        imgs, targets = batch
        imgs = imgs.to(self.device)
        targets = {k: v.to(self.device) for k, v in targets.items()}

        # å‰å‘ä¼ æ’­
        self.optimizer.zero_grad()
        with torch.amp.autocast(
            device_type=self.device.type, enabled=self.device.type == "cuda"
        ):
            outputs = self.model(imgs)
            losses = self.criterion(outputs, targets)
            loss = losses["total"]

        # åå‘ä¼ æ’­
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"\nâš ï¸ NaN/Inf loss detected, skipping batch")
            return {k: 0.0 for k in losses.keys()}

        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=self.config.training.get("grad_clip", 1.0),
        )
        self.optimizer.step()
        self.scheduler.step()

        return {k: v.item() for k, v in losses.items()}

    def _epoch_end(self, epoch: int, avg_losses):
        """é‡å†™ epoch ç»“æŸå¤„ç†ï¼Œæ·»åŠ  curriculum æ›´æ–°"""
        # è°ƒç”¨çˆ¶ç±»çš„ epoch ç»“æŸé€»è¾‘
        super()._epoch_end(epoch, avg_losses)

        # æ›´æ–° curriculum
        self.update_curriculum(epoch + 1)

    def set_dataloader(self, dataloader):
        """è®¾ç½® dataloader å¼•ç”¨ï¼Œç”¨äºå¯è§†åŒ–"""
        self.dataloader = dataloader
        # åˆå§‹åŒ–å¯è§†åŒ–å™¨
        self.visualizer = DenseVisualizer(
            writer=self.writer,
            device=self.device,
            num_samples=4,
        )

    def evaluate(self):
        """åœ¨ TensorBoard ä¸­ç”Ÿæˆå¯è§†åŒ–"""
        if not hasattr(self, "visualizer") or not hasattr(self, "dataloader"):
            return

        metrics = self.visualizer.visualize(
            model=self.model,
            dataloader=self.dataloader,
            global_step=self.global_step,
            prefix="Dense",
        )

        # æ‰“å°æŒ‡æ ‡
        print(f"  ğŸ“Š Eval: IoU={metrics['skel_iou']:.3f}, F1={metrics['skel_f1']:.3f}")


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================


def parse_args():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€è®­ç»ƒè„šæœ¬")

    # é…ç½®
    parser.add_argument("--config", type=str, help="YAML é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--stage", type=str, choices=["structural", "dense"], help="è®­ç»ƒé˜¶æ®µ"
    )
    parser.add_argument(
        "--run-all-stages", action="store_true", help="è‡ªåŠ¨è¿è¡Œæ‰€æœ‰é˜¶æ®µ"
    )

    # Checkpoint
    parser.add_argument("--resume", type=str, help="ä» checkpoint æ¢å¤è®­ç»ƒ")
    parser.add_argument(
        "--init_from", type=str, help="ä» checkpoint åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ–°è®­ç»ƒï¼‰"
    )

    # è¦†ç›–é…ç½®ï¼ˆå¯é€‰ï¼‰
    parser.add_argument("--lr", type=float, help="è¦†ç›–å­¦ä¹ ç‡")
    parser.add_argument("--epochs", type=int, help="è¦†ç›–è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, help="è¦†ç›–æ‰¹æ¬¡å¤§å°")

    return parser.parse_args()


def create_dataloader(config: Config, stage: str):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨

    Args:
        config: é…ç½®å¯¹è±¡
        stage: è®­ç»ƒé˜¶æ®µ ('structural' æˆ– 'dense')

    Returns:
        (dataloader, dataset) å…ƒç»„
    """
    curriculum_stage = config.data.get("curriculum_stage", 0)

    dataset = DenseInkTraceDataset(
        img_size=config.data.get("img_size", 64),
        batch_size=config.training["batch_size"],
        epoch_length=config.training["epoch_length"],
        curriculum_stage=curriculum_stage,
        rust_threads=config.data.get("rust_threads", None),
    )

    num_workers = config.data.get("num_workers", 4)
    dataloader = DataLoader(
        dataset,
        batch_size=config.training["batch_size"],
        num_workers=num_workers,
        collate_fn=collate_dense_batch,
        pin_memory=True,
        persistent_workers=num_workers > 0,
    )

    return dataloader, dataset


def run_single_stage(args, config: Config, stage_name: str, init_from: str = None):
    """è¿è¡Œå•ä¸ªé˜¶æ®µ

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        config: é…ç½®å¯¹è±¡
        stage_name: é˜¶æ®µåç§° ('structural' æˆ– 'dense')
        init_from: åˆå§‹åŒ–æƒé‡è·¯å¾„

    Returns:
        æœ€ä½³ checkpoint è·¯å¾„
    """

    # ä»é…ç½®ä¸­è·å– stage é…ç½®
    stage_config = None
    if config.stages:
        for stage in config.stages:
            if stage["name"] == stage_name:
                stage_config = stage
                break

    if stage_config:
        # åˆå¹¶ stage é…ç½®
        config = Config(
            model={**config.model, **stage_config.get("model", {})},
            training={**config.training, **stage_config.get("training", {})},
            data={**config.data, **stage_config.get("data", {})},
            logging={**config.logging, **stage_config.get("logging", {})},
            device=config.device,
        )
        if init_from is None:
            init_from = stage_config.get("init_from")

    # åˆ›å»º trainer
    if stage_name == "structural":
        trainer = StructuralTrainer(config, init_from=init_from)
        dataloader, _ = create_dataloader(config, stage_name)
    elif stage_name == "dense":
        trainer = DenseTrainer(config, init_from=init_from)
        dataloader, dataset = create_dataloader(config, stage_name)
        # å°† dataset ä¼ é€’ç»™ trainerï¼Œæ”¯æŒåŠ¨æ€ curriculum
        trainer.set_dataset(dataset)
        # è®¾ç½® dataloader ç”¨äºå¯è§†åŒ–
        trainer.set_dataloader(dataloader)
    else:
        raise ValueError(f"Unknown stage: {stage_name}")

    # æ¢å¤è®­ç»ƒ
    if args.resume:
        trainer.load_checkpoint(args.resume)

    # è®­ç»ƒ
    trainer.train(dataloader)

    return trainer.ckpt_manager.save_dir / "checkpoint_best.pth"


def main():
    args = parse_args()

    # åŠ è½½é…ç½®
    if args.config:
        config = Config.from_yaml(args.config)
    else:
        # å‘åå…¼å®¹ï¼šä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®
        config = Config.from_args(args)

    # è¦†ç›–é…ç½®
    if args.lr:
        config.training["lr"] = args.lr
    if args.epochs:
        config.training["epochs"] = args.epochs
    if args.batch_size:
        config.training["batch_size"] = args.batch_size

    # è¿è¡Œè®­ç»ƒ
    if args.resume:
        # æ¢å¤è®­ç»ƒ - è‡ªåŠ¨æ£€æµ‹ stage
        ckpt_config = Config.from_checkpoint(args.resume)
        stage = ckpt_config.metadata.get("stage", "dense")
        run_single_stage(args, config, stage, init_from=None)

    elif args.run_all_stages:
        # è¿è¡Œæ‰€æœ‰é˜¶æ®µ
        if not config.stages:
            raise ValueError("No stages defined in config")

        last_ckpt = None
        for stage in config.stages:
            stage_name = stage["name"]
            print(f"\n{'#' * 60}")
            print(f"# Running stage: {stage_name}")
            print(f"{'#' * 60}\n")

            # init_from æŒ‡å®šæˆ–ä½¿ç”¨ä¸Šä¸€ä¸ªé˜¶æ®µçš„ checkpoint
            init_from = stage.get("init_from")
            if init_from and last_ckpt:
                init_from = init_from.replace("*", str(last_ckpt))
            elif not init_from and last_ckpt:
                init_from = str(last_ckpt)

            best_ckpt = run_single_stage(args, config, stage_name, init_from=init_from)
            last_ckpt = best_ckpt

    else:
        # è¿è¡Œå•ä¸ªé˜¶æ®µ
        if not args.stage:
            raise ValueError("--stage or --run-all-stages is required")
        run_single_stage(args, config, args.stage, init_from=args.init_from)


if __name__ == "__main__":
    main()
